{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2\\\\SFT_LoRA_QLoRA_RLHF'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LORA Supervised Finetunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the LORA SFT from this paper [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gpt import GPTModel, MultiHeadAttention, FeedForward, LayerNorm\n",
    "from model_args import BASE_CONFIG\n",
    "from utils.load_and_save_models import load_model, save_model\n",
    "from utils.train import finetune_model, dataloaders\n",
    "from utils.download_dataset import download_and_load_dataset, partition_data\n",
    "from utils.generate import generate, generate_and_print_text\n",
    "from utils.token_converter import get_tokenizer, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the LORA adapter layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LoraLinear(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "        custom LoRA implementation for linear layers\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, linear_layer, r=8, lora_alpha=16, lora_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear_layer = linear_layer\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = lora_dropout\n",
    "\n",
    "        # Getting the original linear layer dimensions\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "\n",
    "        # creating the LoRA adapters\n",
    "        self.lora_A = nn.Linear(self.in_features, self.r, bias=False)\n",
    "        self.lora_B = nn.Linear(self.r, self.out_features, bias=False)\n",
    "\n",
    "        # Initialize LoRA weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)  \n",
    "\n",
    "        self.scaling = self.lora_alpha / self.r\n",
    "\n",
    "    def forward(self, x):\n",
    "        # original forward pass\n",
    "        original_output = self.linear_layer(x)  \n",
    "\n",
    "        #LoRA forward pass\n",
    "        lora_output = self.lora_B(self.lora_A(F.dropout(x, self.lora_dropout, self.training)))\n",
    "\n",
    "        # combine both the outputs\n",
    "        return original_output + self.scaling * lora_output\n",
    "\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAMultiHeadAttention(MultiHeadAttention):\n",
    "    \"\"\"\n",
    "    MultiHeadAttention with LoRA applied to query, key, and value projections\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False, r=8, lora_alpha=16, lora_dropout=0.1):\n",
    "        super().__init__(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n",
    "\n",
    "        # Apply LoRA to query, key and value projections\n",
    "        self.W_query = LoraLinear(self.W_query, r, lora_alpha, lora_dropout)\n",
    "        self.W_key = LoraLinear(self.W_key, r, lora_alpha, lora_dropout)\n",
    "        self.W_value = LoraLinear(self.W_value, r, lora_alpha, lora_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRATransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with LoRA applied to the attention layers\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, r=8, lora_alpha=16, lora_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.att = LoRAMultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAGPTModel(GPTModel):\n",
    "    \"\"\"\n",
    "    GPT model with LoRA applied to attention layers\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, r=8, lora_alpha=16, lora_dropout=0.1):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        # Replacing the transformer blocks with the Lora Blocks\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[LoRATransformerBlock(cfg, r, lora_alpha, lora_dropout)\n",
    "             for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        \"\"\" Get only the LoRA parameters to train\"\"\"\n",
    "        trainable_params = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"lora_A\" in name or \"lora_B\" in name:\n",
    "                trainable_params.append(param)\n",
    "        return trainable_params\n",
    "\n",
    "    def print_trainable_parameters(self):\n",
    "\n",
    "        trainable_params = self.get_trainable_parameters()\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params_count = sum(p.numel() for p in trainable_params)\n",
    "\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params_count:,}\")\n",
    "        print(f\"Percentage of trainable parameters: {100 * trainable_params_count / total_params:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lora_model(base_model_path, r=8, lora_alpha=16, lora_dropout=0.1):\n",
    "    \"\"\"\n",
    "    creating the LoRA enabled gpt model from pretrained model\n",
    "    \"\"\"\n",
    "\n",
    "    # creating the lora model\n",
    "    lora_model = LoRAGPTModel(BASE_CONFIG, r, lora_alpha, lora_dropout)\n",
    "\n",
    "    print(f\"Loading model from the  :{ base_model_path}\")\n",
    "    state_dict = torch.load(base_model_path, map_location=\"cpu\")\n",
    "\n",
    "    # Load the weights that dont have LoRA (embedding, norm, output layers)\n",
    "    model_dict = lora_model.state_dict()\n",
    "    for name, param in state_dict.items():\n",
    "        if name in model_dict and \"lora\" not in name:\n",
    "            model_dict[name] = param\n",
    "\n",
    "    lora_model.load_state_dict(model_dict)\n",
    "    print(\"model weights are loaded\")\n",
    "\n",
    "    # freezing the base model parameters\n",
    "    for name, param in lora_model.named_parameters():\n",
    "        if \"lora\" not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    return lora_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LORA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from the  :D:\\software_3\\Generative_models\\Text_models\\chat_gpt2\\gpt_models\\Foundational_model.pth\n",
      "model weights are loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoRAGPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): LoRATransformerBlock(\n",
       "      (att): LoRAMultiHeadAttention(\n",
       "        (W_query): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_key): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (W_value): LoraLinear(\n",
       "          (linear_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (lora_A): Linear(in_features=768, out_features=8, bias=False)\n",
       "          (lora_B): Linear(in_features=8, out_features=768, bias=False)\n",
       "        )\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_path = \"GPT2-355M-pretrained.pth\"\n",
    "model = create_lora_model(pretrained_model_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful! Output shape: torch.Size([2, 10, 50257])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    batch_size, seq_len = 2, 10\n",
    "    dummy_input = torch.randint(0, BASE_CONFIG[\"vocab_size\"], (batch_size, seq_len)).to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "        \n",
    "    print(f\"Forward pass successful! Output shape: {output.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 935\n",
      "Test data length: 110\n",
      "Validation data length: 55\n"
     ]
    }
   ],
   "source": [
    "data = download_and_load_dataset(\"instruction-data.json\")\n",
    "tokenizer = get_tokenizer()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data, test_data, val_data = partition_data(data)\n",
    "train_dataloader, test_dataloader, val_dataloader = dataloaders(train_data, val_data, test_data, tokenizer=tokenizer,batch_size=16)\n",
    "optimizer = torch.optim.AdamW(model.get_trainable_parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 59/59 [00:43<00:00,  1.35it/s, train_loss=2.703, val_loss=2.732]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'                                    The first of a- The first of a- The first step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 59/59 [00:44<00:00,  1.33it/s, train_loss=2.539, val_loss=2.568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'                                         The first step. The first step. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 59/59 [00:45<00:00,  1.29it/s, train_loss=2.423, val_loss=2.460]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 3] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'      ### Response:  ### Response:       The first step by the following the following the following the word 'The first step by the following the following the following the following the following the word '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 59/59 [00:43<00:00,  1.35it/s, train_loss=2.321, val_loss=2.371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 4] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'     ### Response: ### Response: ### Response: The first step by the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 59/59 [00:40<00:00,  1.44it/s, train_loss=2.244, val_loss=2.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 5] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response: ### Response: The first step by the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 59/59 [00:44<00:00,  1.34it/s, train_loss=2.282, val_loss=2.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 6] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: ### Response: The first step. The first step. The first step. The first step. The first step. The first step. The first step. The first. The first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 59/59 [00:44<00:00,  1.31it/s, train_loss=2.197, val_loss=2.252]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 7] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: ### Response: The first step. The first step. The first step. The first step. The first. The first. The first. The first. The first. The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 59/59 [00:45<00:00,  1.28it/s, train_loss=2.131, val_loss=2.192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 8] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response: The first step. The first step. The first. The first. The first. The first. The first. The first. The first. The first. The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 59/59 [00:44<00:00,  1.33it/s, train_loss=2.078, val_loss=2.147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 9] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response: The following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the following the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 59/59 [00:41<00:00,  1.42it/s, train_loss=2.040, val_loss=2.112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 10] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response:  The following the following the following the following the following the following the following the following the following the following the following the following the following the world is 'The city.         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 59/59 [00:44<00:00,  1.34it/s, train_loss=2.000, val_loss=2.079]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 11] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response:      The following the city of the world is 'the same as a.                     The city of the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 59/59 [00:44<00:00,  1.32it/s, train_loss=1.964, val_loss=2.046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 12] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response:     The following the city of the city.                 The city of the city of the city of the world is the world\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 59/59 [00:45<00:00,  1.28it/s, train_loss=1.926, val_loss=2.018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 13] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response:     ### Response: The city of the city of the city of the world is 'I'mo.            The city of the world is '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 59/59 [00:45<00:00,  1.31it/s, train_loss=1.893, val_loss=1.988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 14] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response:   ### Response: The book is 'I'mo' is 'I'mo' is 'I'mo' is 'I'mo' is 'I'mo' is 'I'mo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 59/59 [00:41<00:00,  1.43it/s, train_loss=1.855, val_loss=1.959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 15] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response:  The following the game.  ### Response: The book is 'I am I am I am I am I am I am I amelvese.   ### Response: The following the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 59/59 [00:45<00:00,  1.31it/s, train_loss=1.818, val_loss=1.926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 16] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The book is 'I amorous' is 'I amorous' is 'I'mo' is 'I'mo' is 'I'mo' is 'I'mo' is 'I'mo'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 59/59 [00:44<00:00,  1.32it/s, train_loss=1.787, val_loss=1.898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 17] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The book is 'I amorous' is 'I amigo'.   ### Response: The book is 'I'mo' is 'I'mo' is 'I'mo' is 'I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 59/59 [00:45<00:00,  1.30it/s, train_loss=1.759, val_loss=1.871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 18] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The book is 'I amo. ### Response: The book is 'I amo. The book is 'I'mo' is 'I'mo' is 'I'mo' is '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 59/59 [00:44<00:00,  1.33it/s, train_loss=1.730, val_loss=1.848]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 19] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The book is 'I amo. ### Response: The book is 'I amo. The book is 'I amigo'. The book is 'I amigo'. The book is '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 59/59 [00:41<00:00,  1.44it/s, train_loss=1.705, val_loss=1.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 20] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The city. The book is 'I amo. ### Response: The book is 'I amo. The book is 'I amo. The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 59/59 [00:43<00:00,  1.35it/s, train_loss=1.679, val_loss=1.811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 21] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The city. ### Response: The city. The city. The city. The city. The city. The city. The city. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 59/59 [00:45<00:00,  1.31it/s, train_loss=1.653, val_loss=1.794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 22] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The city. The book is 'I amo. ### Response: The city. The city. The city. The city. The city.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 59/59 [00:45<00:00,  1.29it/s, train_loss=1.637, val_loss=1.782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 23] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The city. The book is 'I amo. ### Response: The book is 'I amo. The book is 'I amo. The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 59/59 [00:44<00:00,  1.33it/s, train_loss=1.609, val_loss=1.757]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 24] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The city. The book is 'I amo. The city. The book. The book. The book. The book. The book,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 59/59 [00:41<00:00,  1.43it/s, train_loss=1.592, val_loss=1.747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 25] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The city. The book is 'I amo. The book. The book. The book. The book. The book. The book.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 59/59 [00:43<00:00,  1.35it/s, train_loss=1.569, val_loss=1.731]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 26] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The book is 'I amo. The book is 'I amo' is a lotus, but I amo' is a lotus, but I amo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 59/59 [00:48<00:00,  1.21it/s, train_loss=1.550, val_loss=1.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 27] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The book is 'I amo. The Great Britain. The book. The book. The book. The book. The book. The book\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 59/59 [00:45<00:00,  1.29it/s, train_loss=1.534, val_loss=1.704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 28] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city of the city. The city. The book is a great. The Great Britain. The Great Britain. The book. The Great Britain. The book. The book.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 59/59 [00:44<00:00,  1.32it/s, train_loss=1.518, val_loss=1.699]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 29] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The city. The book is a great. The Great Britain. The Great Britain. The Great Britain. The Great Britain. The Great Britain. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 59/59 [00:41<00:00,  1.43it/s, train_loss=1.531, val_loss=1.718]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 30] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The city. The city. The Great Britain. The book is a man. The Great Britain. The Great Britain. The Great Britain. The Great Britain. The Great Britain.\n",
      "Model training has been completed.\n",
      "training completed in 22.96 minutes.\n"
     ]
    }
   ],
   "source": [
    "lora_sft_model = finetune_model(\n",
    "                 model=model,\n",
    "                 train_loader=train_dataloader,\n",
    "                 val_loader=val_dataloader,\n",
    "                 optimizer=optimizer,\n",
    "                 device=device,\n",
    "                 num_epochs=num_epochs,\n",
    "                 tokenizer=tokenizer,\n",
    "                 val_data=val_data,\n",
    "                 eval_freqs=5,\n",
    "                 eval_iter=5,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model has been saved successfully at d:\\software_3\\Generative_models\\Text_models\\chat_gpt2\\gpt_models\\instruction_finetunned_LoRA_model.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(lora_sft_model, \"instruct-LORA-GPT2-355M.pth\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert 45 kilometers to meters.00:\n",
      "The following:\n",
      "The following:\n",
      "The following\n",
      "The following\n",
      "2.\n",
      "2.\n",
      "2.\n",
      "2.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "text = \"Convert 45 kilometers to meters.\"\n",
    "\n",
    "encoded_text =  text_to_token_ids(text, tokenizer).to(device)\n",
    "\n",
    "idx = encoded_text\n",
    "token_ids = generate(\n",
    "        model=lora_sft_model,\n",
    "        idx=encoded_text,\n",
    "        max_new_tokens=30,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        temperature=0.0,\n",
    "        top_k=None,\n",
    "        eos_id=None\n",
    "    )\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
