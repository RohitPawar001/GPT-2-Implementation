{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2\\\\SFT_LoRA_QLoRA_RLHF'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLORA Supervised Fine Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the QLORA SFT from this paper [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) \n",
    "\n",
    "The QLORA SFT is an quantized adapter model which means the model weights are quantized into the smaller memeory size\n",
    "the main advantage of this approach is that while finetunning we does not need to finetune the entire model instead we can only finetune the only the adapter layers resulting into faster training and can work with the consumer hardwares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gpt import GPTModel, MultiHeadAttention, LayerNorm, FeedForward\n",
    "from model_args import BASE_CONFIG\n",
    "from utils.load_and_save_models import load_model, save_model\n",
    "from utils.train import finetune_model, dataloaders\n",
    "from utils.download_dataset import download_and_load_dataset, partition_data\n",
    "from utils.generate import generate,\n",
    "from utils.token_converter import get_tokenizer, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the QLORA GPTMODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Quantized Linear layers using 4-bit quantization\n",
    "    \"\"\"\n",
    "    def __init__(self, linear_layer, bits=4):\n",
    "        super().__init__()\n",
    "        self.linear = linear_layer\n",
    "        self.bits = bits\n",
    "        self.scale = None\n",
    "        self.zero_point = None\n",
    "\n",
    "    def quantize_weights(self):\n",
    "        # quantize weights to 4 bits\n",
    "        weights = self.linear.weight.data\n",
    "\n",
    "        # calculate scale and zero point for quantization\n",
    "        w_min, w_max = weights.min(), weights.max()\n",
    "        self.scale = (w_max - w_min) / (2**self.bits - 1)\n",
    "        self.zero_point = w_min\n",
    "\n",
    "        # Quantize weights\n",
    "        quantized = torch.round((weights - self.zero_point) / self.scale)\n",
    "        quantized = torch.clamp(quantized, 0, 2**self.bits - 1)\n",
    "\n",
    "        # store quantized weights\n",
    "        self.register_buffer(\"quantized_weights\", quantized)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not hasattr(self, \"quantized_weights\"):\n",
    "            self.quantize_weights()\n",
    "\n",
    "        # Dequantize weights for the forward pass\n",
    "        dequantized_weights = self.quantized_weights * self.scale + self.zero_point\n",
    "\n",
    "        # use dequantized weights for computation\n",
    "        return F.linear(x, dequantized_weights, self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRALinear(nn.Module):\n",
    "    \"\"\"QLoRA implementation (quantized base layer + lora adapter)\"\"\"\n",
    "    def __init__(self, linear_layer, r=8, lora_alpha=16, lora_dropout=0.1, bits=4):\n",
    "        super().__init__()\n",
    "        self.quantized_linear = QuantizedLinear(linear_layer, bits)\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = lora_dropout\n",
    "\n",
    "        self.in_features = linear_layer.in_features\n",
    "        self.out_features = linear_layer.out_features\n",
    "\n",
    "        # creating the LoRA adapters (in full dimension)\n",
    "        self.lora_A = nn.Linear(self.in_features, self.r, bias=False)\n",
    "        self.lora_B = nn.Linear(self.r, self.out_features, bias=False)\n",
    "\n",
    "        # initialize the lora weights\n",
    "        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B.weight)\n",
    "\n",
    "        # scaling factor\n",
    "        self.scaling = self.lora_alpha / self.r\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Quantized forward pass\n",
    "        quantized_output = self.quantized_linear(x)\n",
    "\n",
    "        # Lora forward pass full precision\n",
    "        lora_output = self.lora_B(self.lora_A(F.dropout(x, self.lora_dropout, self.training)))\n",
    "\n",
    "        # combine quantized and lora weights\n",
    "        return quantized_output + self.scaling * lora_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRAMultiHeadAttention(MultiHeadAttention):\n",
    "    \"\"\"\n",
    "    MultiHeadAttention with QLoRA applied to q, k, v projections\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False,\n",
    "                 r=8, lora_alpha=16, lora_dropout=0.1, bits=4):\n",
    "        super().__init__(d_in, d_out, context_length, dropout, num_heads, qkv_bias)\n",
    "\n",
    "        self.W_query = QLoRALinear(self.W_query, r, lora_alpha, lora_dropout, bits)\n",
    "        self.W_key = QLoRALinear(self.W_key, r, lora_alpha, lora_dropout, bits)\n",
    "        self.W_value = QLoRALinear(self.W_value, r, lora_alpha, lora_dropout, bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRATransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block with QLoRA applied to attention layers\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, r=8, lora_alpha=16, lora_dropout=0.1, bits=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.att = QLoRAMultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"],\n",
    "            r=r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            bits=bits\n",
    "        )\n",
    "\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLoRAGPTModel(GPTModel):\n",
    "    \"\"\"\n",
    "    GPT model with QLoRA applied to attention layers\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, r=8, lora_alpha=16, lora_dropout=0.1, bits=4):\n",
    "        super().__init__(cfg)  # Fixed: was super.__init__(cfg)\n",
    "\n",
    "        # Replace transformer blocks with QLoRA versions\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[QLoRATransformerBlock(cfg, r, lora_alpha, lora_dropout, bits)\n",
    "            for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "    def get_trainable_parameters(self):\n",
    "        # get only lora trainable parameters\n",
    "        trainable_params = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"lora_A\" in name or \"lora_B\" in name:\n",
    "                trainable_params.append(param)\n",
    "        return trainable_params\n",
    "\n",
    "    def print_trainable_parameters(self):\n",
    "        \"\"\"Print information about trainable parameters\"\"\"\n",
    "        trainable_params = self.get_trainable_parameters()\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params_count = sum(p.numel() for p in trainable_params)\n",
    "        \n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters (LoRA only): {trainable_params_count:,}\")\n",
    "        print(f\"Percentage of trainable parameters: {100 * trainable_params_count / total_params:.2f}%\")\n",
    "        print(f\"Memory savings: Base model is quantized to {self.trf_blocks[0].att.W_query.quantized_linear.bits}-bit precision\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qlora_model(base_model_path, r=8, lora_alpha=16, lora_dropout=0.1, bits=4):\n",
    "    \"\"\"\n",
    "    Create a QLoRA-enabled GPT model from a pre-trained model\n",
    "    \"\"\"\n",
    "    qlora_model = QLoRAGPTModel(BASE_CONFIG, r, lora_alpha, lora_dropout, bits)\n",
    "\n",
    "    print(\"Loading base model\")\n",
    "    state_dict = torch.load(base_model_path, map_location=\"cpu\")\n",
    "\n",
    "    # Load weights that don't have LoRA (embedding, norm, output layers)\n",
    "    model_dict = qlora_model.state_dict()\n",
    "    for name, param in state_dict.items():\n",
    "        if name in model_dict and \"lora\" not in name and \"quantized\" not in name:\n",
    "            model_dict[name] = param  # Fixed: was model_dict[param] = param\n",
    "\n",
    "    qlora_model.load_state_dict(model_dict)\n",
    "    print(\"Model weights loaded successfully\")\n",
    "\n",
    "    # Freeze the base model parameters (quantized layers)\n",
    "    for name, param in qlora_model.named_parameters():\n",
    "        if \"lora\" not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    return qlora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_memory_usage(model, bits=4):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.get_trainable_parameters())\n",
    "    \n",
    "    # Estimate memory usage (rough calculation)\n",
    "    # Base model: quantized to 4-bit\n",
    "    base_memory = (total_params - trainable_params) * bits / 8  # bytes\n",
    "    # LoRA parameters: full precision (32-bit)\n",
    "    lora_memory = trainable_params * 4  # bytes\n",
    "    \n",
    "    total_memory_mb = (base_memory + lora_memory) / (1024 * 1024)\n",
    "    \n",
    "    print(f\"Estimated memory usage: {total_memory_mb:.2f} MB\")\n",
    "    print(f\"  - Base model (quantized): {base_memory / (1024 * 1024):.2f} MB\")\n",
    "    print(f\"  - LoRA adapters: {lora_memory / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model\n",
      "Model weights loaded successfully\n",
      "####################################\n",
      "Estimated memory usage: 79.43 MB\n",
      "  - Base model (quantized): 77.74 MB\n",
      "  - LoRA adapters: 1.69 MB\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_path = \"gpt_models\\\\GPT2-355M-pretrained.pth\"\n",
    "model = create_qlora_model(pretrained_model_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(\"####################################\")\n",
    "estimate_memory_usage(model, bits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful! Output shape: torch.Size([2, 10, 50257])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    batch_size, seq_len = 2, 10\n",
    "    dummy_input = torch.randint(0, BASE_CONFIG[\"vocab_size\"], (batch_size, seq_len)).to(device)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "        \n",
    "    print(f\"Forward pass successful! Output shape: {output.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data length: 935\n",
      "Test data length: 110\n",
      "Validation data length: 55\n"
     ]
    }
   ],
   "source": [
    "data = download_and_load_dataset(\"instruction-data.json\")\n",
    "tokenizer = get_tokenizer()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_data, test_data, val_data = partition_data(data)\n",
    "train_dataloader, test_dataloader, val_dataloader = dataloaders(train_data, val_data, test_data, tokenizer=tokenizer,batch_size=16)\n",
    "optimizer = torch.optim.AdamW(model.get_trainable_parameters(), lr=0.0004, weight_decay=0.1)\n",
    "num_epochs = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 59/59 [01:19<00:00,  1.34s/it, train_loss=5.152, val_loss=5.065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'                                                                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 59/59 [01:24<00:00,  1.44s/it, train_loss=4.073, val_loss=4.013]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'                                                                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 59/59 [01:02<00:00,  1.05s/it, train_loss=3.269, val_loss=3.255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 3] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'                                                                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 59/59 [01:36<00:00,  1.63s/it, train_loss=2.814, val_loss=2.822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 4] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'                                                                                                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 59/59 [01:00<00:00,  1.02s/it, train_loss=2.571, val_loss=2.597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 5] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'                                                                   The 'the.      The 'the. The 'the. The 'the. The 'the. The 'the.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 59/59 [01:14<00:00,  1.27s/it, train_loss=2.394, val_loss=2.440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 6] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'               The first.   The first.  The first.        The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The 'the. The 'the. The 'the. The 'the. The 'the.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 59/59 [01:00<00:00,  1.02s/it, train_loss=2.306, val_loss=2.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 7] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'                   The word is the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same. The 'the 'the. The 'the. The 'the. The 'the. The 'the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 59/59 [01:35<00:00,  1.61s/it, train_loss=2.240, val_loss=2.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 8] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'             The first.  The first.   The first.    The first.  The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The 'the. The 'the. The 'the. The 'the. The 'the. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 59/59 [01:35<00:00,  1.62s/it, train_loss=2.205, val_loss=2.266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 9] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response: ### Response: The word 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the same as a 'the. The Great. The Great. The Great. The Great. The Great. The Great.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 59/59 [01:02<00:00,  1.06s/it, train_loss=2.149, val_loss=2.213]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 10] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'     ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The Great. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 59/59 [01:40<00:00,  1.70s/it, train_loss=2.096, val_loss=2.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 11] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 59/59 [01:03<00:00,  1.07s/it, train_loss=2.056, val_loss=2.118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 12] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 59/59 [01:18<00:00,  1.33s/it, train_loss=2.033, val_loss=2.095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 13] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 59/59 [01:01<00:00,  1.04s/it, train_loss=1.994, val_loss=2.053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 14] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'    ### Response: The first. The first. The first. The first. The first. The first.  The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 59/59 [01:37<00:00,  1.66s/it, train_loss=1.969, val_loss=2.026]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 15] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. The first.  The first.  The first.  The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 59/59 [00:59<00:00,  1.02s/it, train_loss=1.936, val_loss=1.996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 16] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first.  The first.  The first.  The first.  The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 59/59 [01:17<00:00,  1.31s/it, train_loss=1.906, val_loss=1.970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 17] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. The first.  The first.  The first. The first.  The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 59/59 [01:10<00:00,  1.20s/it, train_loss=1.876, val_loss=1.944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 18] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. The first. The first.  The first.  The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 59/59 [01:12<00:00,  1.24s/it, train_loss=1.847, val_loss=1.919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 19] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. The first. The first.   The first. The first.  The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. I am I am I am I am I am I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 59/59 [01:03<00:00,  1.08s/it, train_loss=1.823, val_loss=1.898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 20] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. The first. The first.  The first.  The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. I am I am I am I am I am I am I am I am\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 59/59 [01:00<00:00,  1.03s/it, train_loss=1.797, val_loss=1.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 21] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. The first. ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The 'I am I am I am I am I am I am I am I am I am I am\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 59/59 [01:28<00:00,  1.50s/it, train_loss=1.774, val_loss=1.861]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 22] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The 'I amigo. I amigo. I amigo. I amigo. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 59/59 [01:02<00:00,  1.06s/it, train_loss=1.754, val_loss=1.846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 23] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. The first. I amigo. I amigo. I amigo. I amigo. I am\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 59/59 [01:14<00:00,  1.27s/it, train_loss=1.730, val_loss=1.826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 24] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. The first. ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. I amigo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 59/59 [01:02<00:00,  1.06s/it, train_loss=1.707, val_loss=1.807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 25] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. I amigo. I am\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 59/59 [01:27<00:00,  1.49s/it, train_loss=1.688, val_loss=1.793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 26] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first. The first. The first. The first. ### Response: The first. The first. The first. The first. The first. The first. The first. The first. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. I amigo. I am\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 59/59 [01:00<00:00,  1.03s/it, train_loss=1.667, val_loss=1.778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 27] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The first day. The first. The first. The first. The first. ### Response: The first. The first. The book. The book. The book. The book. The book. The book. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. The '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 59/59 [01:27<00:00,  1.49s/it, train_loss=1.648, val_loss=1.763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 28] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The book. The book. The next to the book. The book. ### Response: The book. The book. The book. The book. The book. The book. The book. The book. The book. The book. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. The 'I amigo. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 59/59 [00:56<00:00,  1.04it/s, train_loss=1.630, val_loss=1.750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 29] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The book. He was a good. The book. The next to the book. ###A lot. The book. The book. The book. The book. The book. The book. The book. The book. The book. The book. The book. The day. The 'I amigo. The day. The day. The day. The day. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 59/59 [01:06<00:00,  1.12s/it, train_loss=1.612, val_loss=1.739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 30] Sample Generation:\n",
      "Below is an instruction that describes a task. Write a response that appropriately complates the request.  ## Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'   ### Response: The book. The book. He is 'I am I am I am I am I am I am I amigo. ###A. The book. The book. The book. The book. The next to the day. The 'I amigo. The 'I amigo. The 'I amigo. The 's. The day. The day. The day. The day\n",
      "Model training has been completed.\n",
      "training completed in 38.59 minutes.\n"
     ]
    }
   ],
   "source": [
    "qlora_sft_model = finetune_model(\n",
    "                 model=model,\n",
    "                 train_loader=train_dataloader,\n",
    "                 val_loader=val_dataloader,\n",
    "                 optimizer=optimizer,\n",
    "                 device=device,\n",
    "                 num_epochs=num_epochs,\n",
    "                 tokenizer=tokenizer,\n",
    "                 val_data=val_data,\n",
    "                 eval_freqs=5,\n",
    "                 eval_iter=5,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model has been saved successfully at d:\\software_3\\Generative_models\\Text_models\\chat_gpt2\\gpt_models\\instruction_finetunned_QLoRA_model.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(qlora_sft_model, \"instruct-QLORA-GPT2-355M.pth\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert 45 kilometers to meters.\n",
      "\n",
      "###\n",
      "The first, and the following the following the following the following the following the following the following the following the sun.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"Convert 45 kilometers to meters.\"\n",
    "\n",
    "encoded_text =  text_to_token_ids(text, tokenizer).to(device)\n",
    "\n",
    "idx = encoded_text\n",
    "token_ids = generate(\n",
    "        model=qlora_sft_model,\n",
    "        idx=encoded_text,\n",
    "        max_new_tokens=30,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        temperature=0.0,\n",
    "        top_k=None,\n",
    "        eos_id=None\n",
    "    )\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
