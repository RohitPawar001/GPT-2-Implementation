{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "492c9e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2\\\\RLHF_DPO_Preference_alignment\\\\RLHF_with_GRPO'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a2651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444e3324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2fa908",
   "metadata": {},
   "source": [
    "# Grop Relative Policy Optimization (GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee36bd6",
   "metadata": {},
   "source": [
    "This Notebook implements the GRPO RLHF tunning based on this [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/abs/2402.03300) research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f421ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tiktoken\n",
    "import urllib\n",
    "import urllib.request \n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from fileinput import filename\n",
    "from sqlite3 import paramstyle\n",
    "\n",
    "from gpt import GPTModel\n",
    "from model_args import BASE_CONFIG\n",
    "from utils.generate import generate\n",
    "from utils.download_dataset import download_and_load_dataset, partition_data\n",
    "from utils.train import train_model\n",
    "from utils.load_and_save_models import load_model, save_model\n",
    "from utils.token_converter import get_tokenizer, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17f055db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd3bf975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately complates the request.\"\n",
    "        f\"\\n\\n## Instruction:\\n{entry[\"prompt\"]}\"\n",
    "    )\n",
    "    return instruction_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b957cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPODataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        \n",
    "        self.encoded_text = []\n",
    "        for entry in data:\n",
    "            prompt = format_input(entry)\n",
    "\n",
    "            prompt_tokens = text_to_token_ids(prompt, tokenizer)\n",
    "\n",
    "            # Ensure prompt_tokens is a flat tensor\n",
    "            if prompt_tokens.numel() > 0 and prompt_tokens.ndim > 1:\n",
    "                # If it's a nested tensor, flatten it\n",
    "                prompt_tokens = prompt_tokens.flatten()\n",
    "\n",
    "            self.encoded_text.append(prompt_tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_text[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07dce6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fun(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    allowed_max_length=None,\n",
    "    mask_prompt_tokens=True,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    batch_data = {\n",
    "        \"prompt\": []\n",
    "    }\n",
    "\n",
    "    max_length_comman = 0\n",
    "    if batch:\n",
    "        current_max_length = max(len(item) + 1 for item in batch)  # item is a list\n",
    "        max_length_comman = max(max_length_comman, current_max_length)\n",
    "\n",
    "    for item in batch:\n",
    "        prompt = torch.tensor(item)\n",
    "        batch_data[\"prompt\"].append(prompt)\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    padded_prompts = nn.utils.rnn.pad_sequence(batch_data[\"prompt\"], batch_first=True, padding_value=pad_token_id)\n",
    "    if allowed_max_length is not None:\n",
    "        padded_prompts = padded_prompts[:, :allowed_max_length]\n",
    "\n",
    "    batch_data[\"prompt\"] = padded_prompts.to(device)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d700be",
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_collate_fn = partial(\n",
    "    custom_collate_fun,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b974b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    train_portion = int(len(data) * 0.8)\n",
    "    test_portion = int(len(data) * 0.1)\n",
    "    val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "    train_data = data[:train_portion]\n",
    "    test_data = data[train_portion:train_portion + test_portion]\n",
    "    val_data = data[train_portion + test_portion:]\n",
    "\n",
    "    print(\"train dataset length:\", len(train_data))\n",
    "    print(\"length of test data:\", len(test_data))\n",
    "    print(\"length of val data:\", len(val_data))\n",
    "\n",
    "    return train_data, test_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402409ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    val_data,\n",
    "    batch_size,\n",
    "    tokenizer\n",
    "):\n",
    "\n",
    "    num_workers = 0\n",
    "    batch_size = 8\n",
    "\n",
    "    train_data = GRPODataset(train_data, tokenizer)\n",
    "    train_dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers)\n",
    "\n",
    "    test_dataset = GRPODataset(test_data, tokenizer)\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=customized_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    val_dataset = GRPODataset(val_data, tokenizer)\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=customized_collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df48ff12",
   "metadata": {},
   "source": [
    "## GRPO Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1f494",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da7d7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveKLController:\n",
    "\n",
    "    def __init__(self, kl_coef=0.01, target_kl=6.0):\n",
    "        self.kl_coef = kl_coef\n",
    "        self.target_kl = target_kl\n",
    "\n",
    "    def update(self, current_kl, n_steps):\n",
    "        proportional_error = np.clip((current_kl / self.target_kl) - 1.0, -0.2, 0.2)\n",
    "        multiplier = 1.0 + (proportional_error * n_steps / 20.0)\n",
    "        self.kl_coef *= multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9de34913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_logprobs(sequences, scores):\n",
    "    \"\"\"\n",
    "    Calculates log-probabilities of the next tokens for multiple agents in batched format.\n",
    "\n",
    "    Args:\n",
    "        sequences (Tensor): shape (batch_size, seq_len)\n",
    "        scores (Tensor): shape (batch_size, seq_len - 1, vocab_size) — model logits (before softmax)\n",
    "\n",
    "    Returns:\n",
    "        logprobs (Tensor): shape (batch_size, seq_len - 1) — log-probability of each next token\n",
    "    \"\"\"\n",
    "    # Get the next tokens (targets) at each position\n",
    "    next_tokens = sequences[:, 1:]  # shape: (batch_size, seq_len - 1)\n",
    "\n",
    "    # Compute log-softmax over vocab dimension\n",
    "    log_probs = F.log_softmax(scores, dim=-1)  # shape: (batch_size, seq_len - 1, vocab_size)\n",
    "\n",
    "    # Gather the log-prob of the actual next token\n",
    "    logprobs = torch.gather(log_probs, dim=2, index=next_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "    # shape: (batch_size, seq_len - 1)\n",
    "\n",
    "    return logprobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3cc3910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rewards(reward_model, responses, tokenizer, config, device=None):\n",
    "\n",
    "    if device is None:\n",
    "        device = next(reward_model.parameters()).device\n",
    "\n",
    "    rewards = []\n",
    "    with torch.no_grad():\n",
    "        for response in responses:\n",
    "            input_ids = text_to_token_ids(response, tokenizer)\n",
    "            if isinstance(input_ids, list):\n",
    "                input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "            if input_ids.ndim == 1:\n",
    "                input_ids = input_ids.unsqueeze(0)\n",
    "            input_ids = input_ids.to(device)\n",
    "\n",
    "            reward = reward_model(input_ids)\n",
    "\n",
    "            if reward.ndim > 1:\n",
    "                reward = reward[0, -1]\n",
    "\n",
    "            reward = reward.unsqueeze(0)\n",
    "            rewards.append(reward)\n",
    "\n",
    "    rewards = torch.cat(rewards, dim=0)\n",
    "\n",
    "    if getattr(config, \"use_score_norm\", False):\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "\n",
    "    if getattr(config, \"use_score_scaling\", False):\n",
    "        rewards = torch.clamp(rewards, -config.score_clip, config.score_clip)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17e5d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Compute discounted returns for a batch of episodes.\n",
    "    Args:\n",
    "        rewards (Tensor): shape (batch_size, episode_len) or (batch_size,)\n",
    "        gamma (float): discount factor\n",
    "    Returns:\n",
    "        Tensor: discounted returns, shape (batch_size, episode_len) or (batch_size,)\n",
    "    \"\"\"\n",
    "    if rewards.ndim == 1:\n",
    "        # Single-step episode: returns = rewards\n",
    "        return rewards\n",
    "    batch_size, episode_len = rewards.shape\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    for t in reversed(range(episode_len)):\n",
    "        if t == episode_len - 1:\n",
    "            returns[:, t] = rewards[:, t]\n",
    "        else:\n",
    "            returns[:, t] = rewards[:, t] + gamma * returns[:, t + 1]\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5dc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_grpo_advantages(agent_rewards, gamma=0.99, baseline_type=\"mean\", eps=1e-8):\n",
    "    \"\"\"\n",
    "    Compute normalized group-relative advantages for GRPO.\n",
    "    Args:\n",
    "        agent_rewards (List[Tensor]): list of rewards for each agent, each of shape (batch_size,) or (batch_size, episode_len)\n",
    "        ...\n",
    "    \"\"\"\n",
    "    num_agents = len(agent_rewards)\n",
    "    agent_returns = [compute_returns(r, gamma) for r in agent_rewards]  # list of tensors: [batch_size] or [batch_size, episode_len]\n",
    "    returns = torch.stack(agent_returns)\n",
    "    if baseline_type == \"mean\":\n",
    "        group_baseline = returns.mean(dim=0)\n",
    "    elif baseline_type == \"median\":\n",
    "        group_baseline = returns.median(dim=0).values\n",
    "    else:\n",
    "        raise ValueError(\"baseline_type must be 'mean' or 'median'\")\n",
    "    raw_advantages = [agent_returns[i] - group_baseline for i in range(num_agents)]\n",
    "    all_adv = torch.stack(raw_advantages)\n",
    "    flat_adv = all_adv.view(-1)\n",
    "    mean_adv = flat_adv.mean()\n",
    "    std_adv = flat_adv.std()\n",
    "    normalized_advantages = [(adv - mean_adv) / (std_adv + eps) for adv in raw_advantages]\n",
    "    return normalized_advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89124b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_responses(prompts, policy_model, tokenizer):\n",
    "    max_context = 1024\n",
    "    responses = []\n",
    "    all_log_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            if isinstance(prompt, list):\n",
    "                prompt = torch.tensor(prompt, dtype=torch.long)\n",
    "            if prompt.ndim == 1:\n",
    "                prompt = prompt.unsqueeze(0)\n",
    "\n",
    "            prompt = prompt.to(policy_model.tok_emb.weight.device)\n",
    "            max_new_tokens = min(100, max_context - prompt.shape[1])\n",
    "\n",
    "            # Generate a response using your autoregressive sampling method\n",
    "            output = generate(\n",
    "                model=policy_model,\n",
    "                idx=prompt,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                context_size=max_context,\n",
    "                eos_id=50256\n",
    "            )\n",
    "\n",
    "            generated_ids = output[0] if isinstance(output, tuple) else output\n",
    "            if generated_ids.ndim == 1:\n",
    "                generated_ids = generated_ids.unsqueeze(0)\n",
    "\n",
    "            response = token_ids_to_text(generated_ids, tokenizer)\n",
    "            responses.append(response)\n",
    "\n",
    "            logits = policy_model(generated_ids)\n",
    "            if logits.ndim == 2:\n",
    "                logits = logits.unsqueeze(0)\n",
    "\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = generated_ids[..., 1:].contiguous()\n",
    "            logprobs_tensor = F.log_softmax(shift_logits, dim=-1)\n",
    "            logprobs_for_labels = logprobs_tensor.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "            # Mean log-prob over generated tokens\n",
    "            all_log_probs.append(logprobs_for_labels.mean())\n",
    "\n",
    "    return responses, torch.stack(all_log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b190c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def forward_pass(responses, policy_model, tokenizer):\n",
    "    logprobs = []\n",
    "\n",
    "    for response in responses:\n",
    "        # Tokenize the response\n",
    "        input_ids = text_to_token_ids(response, tokenizer)\n",
    "        if isinstance(input_ids, list):\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        if input_ids.ndim == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "        input_ids = input_ids.to(next(policy_model.parameters()).device)\n",
    "\n",
    "        # Forward pass through the policy model\n",
    "        logits = policy_model(input_ids)  # shape: [B, T, Vocab]\n",
    "        if logits.ndim == 2:\n",
    "            logits = logits.unsqueeze(0)\n",
    "\n",
    "        # Align logits and labels for next-token prediction\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "\n",
    "        # Log-softmax for log-probs over vocab\n",
    "        logprobs_tensor = F.log_softmax(shift_logits, dim=-1)\n",
    "\n",
    "        # Gather log-probs of actual tokens\n",
    "        logprobs_for_labels = logprobs_tensor.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Mean log-prob for the response\n",
    "        logprobs.append(logprobs_for_labels.mean())\n",
    "\n",
    "    return torch.stack(logprobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55240e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_update(\n",
    "    policy_model,\n",
    "    policy_optimizer,\n",
    "    kl_ctl,\n",
    "    stats,\n",
    "    tokenizer,\n",
    "    responses,\n",
    "    old_logprobs,\n",
    "    rewards,\n",
    "    advantages,\n",
    "    config\n",
    "):\n",
    "    batch_size = len(responses)\n",
    "    indices = torch.randperm(batch_size)\n",
    "\n",
    "    policy_losses = []\n",
    "    kl_divergence = []\n",
    "\n",
    "    device = next(policy_model.parameters()).device  # <-- Add this line\n",
    "\n",
    "    for epoch in range(config.grpo_epochs):\n",
    "        for i in range(0, batch_size, config.mini_batch_size):\n",
    "            mini_batch_indices = indices[i:i + config.mini_batch_size]\n",
    "\n",
    "            mini_responses = [responses[idx] for idx in mini_batch_indices]\n",
    "            mini_old_logprobs = old_logprobs[mini_batch_indices]\n",
    "            mini_advantages = advantages[mini_batch_indices]\n",
    "\n",
    "            policy_model.train()\n",
    "\n",
    "            new_logprobs, _ = forward_pass(mini_responses, policy_model, tokenizer)\n",
    "\n",
    "            ratio = torch.exp(new_logprobs - mini_old_logprobs)  # π/π_old\n",
    "            surr1 = ratio * mini_advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range) * mini_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            kl_div = (mini_old_logprobs - new_logprobs).mean()\n",
    "\n",
    "            # Value loss (optional — only if using a value head)\n",
    "            value_loss = torch.tensor(0.0, device=device)  # <-- Use device here\n",
    "\n",
    "            total_loss = policy_loss + config.vf_coef * value_loss + kl_ctl.kl_coef * kl_div\n",
    "\n",
    "            policy_optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), config.max_grad_norm)\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            policy_losses.append(policy_loss.item())\n",
    "            kl_divergence.append(kl_div.item())\n",
    "\n",
    "    \n",
    "\n",
    "    # Update KL controller\n",
    "    mean_kl = np.mean(kl_divergence)\n",
    "    kl_ctl.update(mean_kl, batch_size)\n",
    "\n",
    "    # Update training stats\n",
    "    stats['policy_loss'].extend(policy_losses)\n",
    "    stats['kl_divergence'].extend(kl_divergence)\n",
    "    stats['rewards'].extend(rewards.tolist())\n",
    "    stats['advantages'].extend(advantages.tolist())\n",
    "\n",
    "    return {\n",
    "        'policy_loss': np.mean(policy_losses),\n",
    "        'kl_divergence': mean_kl,\n",
    "        'mean_rewards': rewards.mean().item(),\n",
    "        'mean_advantages': advantages.mean().item()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "748a9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    batch,\n",
    "    policy_model,\n",
    "    reward_model,\n",
    "    policy_optimizer,\n",
    "    kl_ctl,\n",
    "    stats,\n",
    "    tokenizer,\n",
    "    config\n",
    "):\n",
    "    # Step 1: Generate responses and logprobs using current policy\n",
    "    prompts = batch['prompt']\n",
    "    responses, old_logprobs = generate_responses(prompts, policy_model, tokenizer)\n",
    "\n",
    "    # Step 2: Compute rewards using the reward model\n",
    "    rewards = compute_rewards(reward_model, responses, tokenizer, config)\n",
    "    # rewards: shape [batch_size]\n",
    "\n",
    "    # Step 3: Compute group-normalized advantages (GRPO specific)\n",
    "    normalized_advantages = compute_normalized_grpo_advantages([rewards])\n",
    "    normalized_advantages = normalized_advantages[0]\n",
    "\n",
    "    # Step 4: Perform GRPO policy update\n",
    "    metrics = grpo_update(\n",
    "        policy_model=policy_model,\n",
    "        policy_optimizer=policy_optimizer,\n",
    "        kl_ctl=kl_ctl,\n",
    "        stats=stats,\n",
    "        tokenizer=tokenizer,\n",
    "        responses=responses,\n",
    "        old_logprobs=old_logprobs,\n",
    "        rewards=rewards,\n",
    "        advantages=normalized_advantages,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a8c3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class GRPOTrainer:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        policy_model,\n",
    "        ref_model,\n",
    "        reward_model,\n",
    "        tokenizer\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.policy_model = policy_model\n",
    "        self.ref_model = ref_model\n",
    "        self.reward_model = reward_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Freeze reference model parameters\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(\n",
    "            self.policy_model.parameters(), lr=self.config.learning_rate\n",
    "        )\n",
    "\n",
    "        self.kl_ctl = AdaptiveKLController(self.config.kl_coef, self.config.kl_target)\n",
    "\n",
    "        self.stats = {\n",
    "            'policy_loss': [],\n",
    "            'kl_divergence': [],\n",
    "            'rewards': [],\n",
    "            'advantages': []\n",
    "        }\n",
    "\n",
    "    def train(self, dataloader, num_epochs=1):\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_metrics = []\n",
    "\n",
    "            for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}\")):\n",
    "                metrics = train_step(\n",
    "                    batch,\n",
    "                    self.policy_model,\n",
    "                    self.reward_model,\n",
    "                    self.policy_optimizer,\n",
    "                    self.kl_ctl,\n",
    "                    self.stats,\n",
    "                    self.tokenizer,\n",
    "                    self.config\n",
    "                )\n",
    "                epoch_metrics.append(metrics)\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    avg_metrics = {k: np.mean([m[k] for m in epoch_metrics[-10:]]) for k in metrics.keys()}\n",
    "                    print(f\"Batch {batch_idx}: {avg_metrics}\")\n",
    "\n",
    "            # Epoch summary\n",
    "            epoch_avg = {k: np.mean([m[k] for m in epoch_metrics]) for k in epoch_metrics[0].keys()}\n",
    "            print(f\"Epoch {epoch+1} Summary: {epoch_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6a06693",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GRPOConfig:\n",
    "    learning_rate: float = 1e-6\n",
    "    batch_size: int = 2\n",
    "    mini_batch_size: int = 2\n",
    "    grpo_epochs: int = 4\n",
    "    max_grad_norm: float = 1.0\n",
    "    clip_range: float = 0.2\n",
    "    clip_range_vf: float = 0.2\n",
    "    vf_coef: float = 0.1\n",
    "    kl_coef: float = 0.01\n",
    "    target_kl: float = 0.1\n",
    "    max_length: int = 512\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.95\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.95\n",
    "    entropy_coef: float = 0.01\n",
    "    adaptive_kl: bool = True\n",
    "    kl_target: float = 6.0\n",
    "    horizon: int = 10000\n",
    "    use_score_scaling: bool = True\n",
    "    use_score_norm: bool = True\n",
    "    score_clip: float = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95fc0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GRPOConfig(\n",
    "    learning_rate=1e-6,\n",
    "    batch_size=2,\n",
    "    grpo_epochs=2,\n",
    "    max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c8542a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcc121c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"gpt_models\\\\SFT_model.pth\"\n",
    "policy_model = GPTModel(BASE_CONFIG)\n",
    "policy_model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(device),\n",
    "    weights_only=True\n",
    "    )['model_state_dict']\n",
    ")\n",
    "policy_model.to(device)\n",
    "policy_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9561198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_model = GPTModel(BASE_CONFIG)\n",
    "reference_model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(device),\n",
    "    weights_only=True\n",
    "    )['model_state_dict']\n",
    ")\n",
    "reference_model.to(device)\n",
    "reference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87c24407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model = GPTModel(BASE_CONFIG)\n",
    "num_classes = 1\n",
    "reward_model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"], \n",
    "    out_features=num_classes\n",
    ")\n",
    "model_path = \"gpt_models\\\\RLHF_reward_model.pth\"\n",
    "reward_model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(device),\n",
    "    weights_only=True\n",
    "    )\n",
    ")\n",
    "reward_model.to(device)\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e79b4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 4000\n",
      "length of test data: 500\n",
      "length of val data: 500\n"
     ]
    }
   ],
   "source": [
    "with open(\"oasst1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "train_data, test_data, val_data = split_data(data)\n",
    "train_dataloader, test_dataloader, val_dataloader = create_dataloaders(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    val_data,\n",
    "    config.batch_size,\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    config,\n",
    "    policy_model,\n",
    "    reference_model,\n",
    "    reward_model,++++\n",
    "    tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/500 [00:00<?, ?it/s]C:\\Users\\rppaw\\AppData\\Local\\Temp\\ipykernel_8632\\1428473432.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prompt = torch.tensor(item)\n",
      "Epoch 1:   0%|          | 1/500 [02:23<19:52:51, 143.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: {'policy_loss': np.float64(0.0), 'kl_divergence': np.float64(0.44504278898239136), 'mean_rewards': np.float64(-2.9802322387695312e-08), 'mean_advantages': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 2/500 [37:23<179:01:19, 1294.14s/it]"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "trainer.train(train_dataloader, num_epochs=NUM_EPOCHS)\n",
    "torch.save(policy_model.state_dict(), 'GRPO_trained_model.pth')\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print()\n",
    "print(\"Training has been completed and model has beed saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
