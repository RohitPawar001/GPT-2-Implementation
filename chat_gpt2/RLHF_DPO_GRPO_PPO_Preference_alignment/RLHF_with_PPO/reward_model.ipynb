{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2\\\\RLHF_DPO_Preference_alignment\\\\RLHF_with_PPO'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Model for the RLHF (ppo, GRPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook implements the Bradly-Terry reward model for the feedback which was from this paper [Bradley–Terry and Multi-Objective Reward Modeling Are Complementary](https://arxiv.org/html/2507.07375v1) this model gives the higher preference to teh chose reward and lower to the rejected rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib\n",
    "from tqdm import tqdm \n",
    "from functools import partial\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from gpt import GPTModel\n",
    "from model_args import BASE_CONFIG\n",
    "from utils.load_and_save_models import save_model\n",
    "from utils.token_converter import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 5000\n"
     ]
    }
   ],
   "source": [
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"preference_reward_model_data.json\"\n",
    "url = \" \"\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rejected': [{'content': {'content': 'Is having your medical records online safe?',\n",
       "    'role': 'user'}},\n",
       "  {'content': {'content': 'You mean being able to share them with your doctor, or making them public to the internet in general?',\n",
       "    'role': 'assistant'}}],\n",
       " 'chosen': [{'content': {'content': 'Is having your medical records online safe?',\n",
       "    'role': 'user'}},\n",
       "  {'content': {'content': 'Hm, I think so! If you have your records online, it makes it easier for you to access them, and also for your doctor to share updates with you, and for you to share updates with your doctor. It also makes it easier for you to share your information with other people, if you want, for example to join a health advocacy or support group. The only risk I see is that if someone else has access to your records, they might abuse that access, or they might accidentally make a change to your records that you don’t want. But overall, I think online access is a good thing.',\n",
       "    'role': 'assistant'}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is having your medical records online safe?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"chosen\"][0][\"content\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \"\"\"Process data to create chosen/rejected pairs with consistent prompt formatting\"\"\"\n",
    "    new_data = []\n",
    "    for i in range(1000):\n",
    "        js = {}\n",
    "        # Extract prompt and responses\n",
    "        prompt = data[i][\"chosen\"][0][\"content\"][\"content\"]\n",
    "        chosen_response = data[i][\"chosen\"][1][\"content\"][\"content\"]\n",
    "        rejected_response = data[i][\"rejected\"][1][\"content\"][\"content\"]\n",
    "        \n",
    "        # Format consistently\n",
    "        js[\"chosen\"] = f\"prompt: {prompt} ### \\n\\n Response: {chosen_response}\"\n",
    "        js[\"rejected\"] = f\"prompt: {prompt} ### \\n\\n Response: {rejected_response}\"\n",
    "        new_data.append(js)\n",
    "    return new_data\n",
    "\n",
    "processed_data = process_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': 'prompt: Is having your medical records online safe? ### \\n\\n Response: Hm, I think so! If you have your records online, it makes it easier for you to access them, and also for your doctor to share updates with you, and for you to share updates with your doctor. It also makes it easier for you to share your information with other people, if you want, for example to join a health advocacy or support group. The only risk I see is that if someone else has access to your records, they might abuse that access, or they might accidentally make a change to your records that you don’t want. But overall, I think online access is a good thing.',\n",
       " 'rejected': 'prompt: Is having your medical records online safe? ### \\n\\n Response: You mean being able to share them with your doctor, or making them public to the internet in general?'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 850\n",
      "Validation set length: 50\n",
      "Test set length: 100\n"
     ]
    }
   ],
   "source": [
    "def split_data(data):\n",
    "    train_portion = int(len(data) * 0.85)\n",
    "    test_portion = int(len(data) * 0.1)\n",
    "    val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "    train_data = data[:train_portion]\n",
    "    test_data = data[train_portion:train_portion + test_portion]\n",
    "    val_data = data[train_portion + test_portion:]\n",
    "    \n",
    "    print(\"Training set length:\", len(train_data))\n",
    "    print(\"Validation set length:\", len(val_data))\n",
    "    print(\"Test set length:\", len(test_data))\n",
    "\n",
    "    return train_data, test_data, val_data\n",
    "\n",
    "train_data, tes_data, val_data = split_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': 'prompt: Is having your medical records online safe? ### \\n\\n Response: Hm, I think so! If you have your records online, it makes it easier for you to access them, and also for your doctor to share updates with you, and for you to share updates with your doctor. It also makes it easier for you to share your information with other people, if you want, for example to join a health advocacy or support group. The only risk I see is that if someone else has access to your records, they might abuse that access, or they might accidentally make a change to your records that you don’t want. But overall, I think online access is a good thing.',\n",
       " 'rejected': 'prompt: Is having your medical records online safe? ### \\n\\n Response: You mean being able to share them with your doctor, or making them public to the internet in general?'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            chosen_full_text = entry[\"chosen\"]\n",
    "            rejected_full_text = entry[\"rejected\"]\n",
    "            \n",
    "            # For tiktoken, encode() returns a list of token IDs directly\n",
    "            chosen_full_tokens = tokenizer.encode(chosen_full_text)\n",
    "            rejected_full_tokens = tokenizer.encode(rejected_full_text)\n",
    "            \n",
    "            # Add EOS token (GPT-2 uses token ID 50256 as EOS)\n",
    "            eos_token_id = 50256\n",
    "            chosen_full_tokens = chosen_full_tokens + [eos_token_id]\n",
    "            rejected_full_tokens = rejected_full_tokens + [eos_token_id]\n",
    "            \n",
    "            # Find prompt end position for masking\n",
    "            prompt_end_marker = \" ### \\n\\n Response:\"\n",
    "            try:\n",
    "                prompt_part = entry[\"chosen\"].split(prompt_end_marker)[0] + prompt_end_marker\n",
    "                prompt_tokens = tokenizer.encode(prompt_part)\n",
    "            except:\n",
    "                # Fallback if split fails\n",
    "                prompt_tokens = tokenizer.encode(entry[\"chosen\"])[:len(tokenizer.encode(entry[\"chosen\"])) // 2]\n",
    "            \n",
    "            self.encoded_texts.append({\n",
    "                \"chosen\": chosen_full_tokens,\n",
    "                \"rejected\": rejected_full_tokens,\n",
    "                \"prompt_len\": len(prompt_tokens)\n",
    "            })\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    allowed_max_length=None,\n",
    "    mask_prompt_tokens=True,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"Fixed collate function with proper masking\"\"\"\n",
    "    \n",
    "    batch_data = {\n",
    "        \"chosen\": [],\n",
    "        \"rejected\": [],\n",
    "        \"rejected_mask\": [],\n",
    "        \"chosen_mask\": []\n",
    "    }\n",
    "\n",
    "    # Determine the longest sequence to set a common padding length\n",
    "    max_length_common = 0\n",
    "    if batch:\n",
    "        for key in [\"chosen\", \"rejected\"]:\n",
    "            current_max = max(len(item[key]) for item in batch)\n",
    "            max_length_common = max(max_length_common, current_max)\n",
    "\n",
    "    # Process each item in the batch\n",
    "    for item in batch:\n",
    "        prompt_len = item[\"prompt_len\"]\n",
    "        \n",
    "        for key in [\"chosen\", \"rejected\"]:\n",
    "            # Adjust the padding according to the common max length\n",
    "            sequence = item[key]\n",
    "            padded = sequence + [pad_token_id] * (max_length_common - len(sequence))\n",
    "            \n",
    "            # Create attention mask (True for real tokens, False for padding)\n",
    "            mask = torch.ones(len(padded)).bool()\n",
    "            mask[len(sequence):] = False  # Set padding tokens to False\n",
    "            \n",
    "            # If mask_prompt_tokens is True, set prompt tokens to False\n",
    "            # We only want to compute loss on the response tokens\n",
    "            if mask_prompt_tokens:\n",
    "                loss_mask = mask.clone()\n",
    "                loss_mask[:prompt_len] = False  # Don't compute loss on prompt tokens\n",
    "                batch_data[f\"{key}_mask\"].append(loss_mask)\n",
    "            else:\n",
    "                batch_data[f\"{key}_mask\"].append(mask)\n",
    "            \n",
    "            batch_data[key].append(torch.tensor(padded))\n",
    "\n",
    "    # Stack all tensors\n",
    "    for key in [\"chosen\", \"rejected\", \"chosen_mask\", \"rejected_mask\"]:\n",
    "        tensor_stack = torch.stack(batch_data[key])\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            tensor_stack = tensor_stack[:, :allowed_max_length]\n",
    "        \n",
    "        batch_data[key] = tensor_stack.to(device)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Encoding' object has no attribute 'eos_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m tokenizer = get_tokenizer()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m example_dataset = \u001b[43mRewardDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m example_dataloader = DataLoader(\n\u001b[32m      6\u001b[39m     example_dataset,\n\u001b[32m      7\u001b[39m     batch_size=\u001b[32m2\u001b[39m,\n\u001b[32m      8\u001b[39m     collate_fn=customized_collate_func,\n\u001b[32m      9\u001b[39m     shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mRewardDataset.__init__\u001b[39m\u001b[34m(self, data, tokenizer)\u001b[39m\n\u001b[32m     10\u001b[39m rejected_full_text = entry[\u001b[33m\"\u001b[39m\u001b[33mrejected\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Add EOS token for proper sequence ending\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m chosen_full_tokens = tokenizer.encode(chosen_full_text) + [\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m]\n\u001b[32m     14\u001b[39m rejected_full_tokens = tokenizer.encode(rejected_full_text) + [tokenizer.eos_token_id]\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Find prompt end position for masking\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Encoding' object has no attribute 'eos_token_id'"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "\n",
    "example_dataset = RewardDataset(val_data, tokenizer)\n",
    "\n",
    "example_dataloader = DataLoader(\n",
    "    example_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn=customized_collate_func,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['chosen', 'rejected', 'rejected_mask', 'chosen_mask'])\n"
     ]
    }
   ],
   "source": [
    "for batch in example_dataloader:\n",
    "    break\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens_from_batch(token_ids, tokenizer):\n",
    "    ids_in_python_list = token_ids.flatten().tolist()\n",
    "    return tokenizer.decode(ids_in_python_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = decode_tokens_from_batch(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     token_ids=\u001b[43mbatch\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mrejected\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m],  \u001b[38;5;66;03m# [0] for the first entry in the batch\u001b[39;00m\n\u001b[32m      3\u001b[39m     tokenizer=tokenizer,\n\u001b[32m      4\u001b[39m )\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(text)\n",
      "\u001b[31mNameError\u001b[39m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "text = decode_tokens_from_batch(\n",
    "    token_ids=batch[\"rejected\"][0],  # [0] for the first entry in the batch\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbatch\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mchosen_mask\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m].shape\n",
      "\u001b[31mNameError\u001b[39m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch[\"chosen_mask\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bradly-terry Loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, batch):\n",
    "    \"\"\"Compute Bradley-Terry loss with proper reward extraction\"\"\"\n",
    "    \n",
    "    # Get logits from the model\n",
    "    chosen_logits = model(batch[\"chosen\"])\n",
    "    rejected_logits = model(batch[\"rejected\"])\n",
    "    \n",
    "    # Extract rewards - take the last non-padded token's reward for each sequence\n",
    "    chosen_rewards = []\n",
    "    rejected_rewards = []\n",
    "    \n",
    "    for i in range(chosen_logits.size(0)):  # For each item in batch\n",
    "        # Find the last non-padded token position\n",
    "        chosen_mask = batch[\"chosen_mask\"][i]\n",
    "        rejected_mask = batch[\"rejected_mask\"][i]\n",
    "        \n",
    "        # Get the last True position (last non-padded token)\n",
    "        chosen_last_pos = chosen_mask.sum() - 1\n",
    "        rejected_last_pos = rejected_mask.sum() - 1\n",
    "        \n",
    "        # Extract reward from the last position\n",
    "        chosen_reward = chosen_logits[i, chosen_last_pos, 0]  # Assuming single output\n",
    "        rejected_reward = rejected_logits[i, rejected_last_pos, 0]\n",
    "        \n",
    "        chosen_rewards.append(chosen_reward)\n",
    "        rejected_rewards.append(rejected_reward)\n",
    "    \n",
    "    chosen_rewards = torch.stack(chosen_rewards)\n",
    "    rejected_rewards = torch.stack(rejected_rewards)\n",
    "    \n",
    "    # Bradley-Terry model: P(chosen > rejected) = σ(r_chosen - r_rejected)\n",
    "    # Loss = -log(σ(r_chosen - r_rejected))\n",
    "    loss = -torch.nn.functional.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "    \n",
    "    return loss, chosen_rewards.mean(), rejected_rewards.mean()\n",
    "\n",
    "\n",
    "def compute_bradley_terry_probability(r_chosen, r_rejected):\n",
    "    \"\"\"Compute P(chosen > rejected) using Bradley-Terry Model\"\"\"\n",
    "    return torch.sigmoid(r_chosen - r_rejected)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reward_model(model, optimizer, device, train_data, val_data, tokenizer, start_context, epochs=3, batch_size=4, lr=5e-5):\n",
    "    \"\"\"Fixed training loop with proper evaluation\"\"\"\n",
    "    \n",
    "    # Create collate function\n",
    "    customized_collate_func = partial(\n",
    "        custom_collate_fn,\n",
    "        device=device,\n",
    "        mask_prompt_tokens=True,\n",
    "        allowed_max_length=1024\n",
    "    )\n",
    "    \n",
    "    train_dataset = RewardDataset(train_data, tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=customized_collate_func)\n",
    "\n",
    "    val_dataset = RewardDataset(val_data, tokenizer)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=customized_collate_func)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_chosen_reward = 0\n",
    "        total_rejected_reward = 0\n",
    "        total_bt_prob = 0\n",
    "\n",
    "        train_iter = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch} [Train]\")\n",
    "        for batch_idx, batch in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss, chosen_reward, rejected_reward = compute_loss(model, batch)\n",
    "            bt_prob = torch.sigmoid(chosen_reward - rejected_reward).item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Add gradient clipping to prevent instability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_chosen_reward += chosen_reward.item()\n",
    "            total_rejected_reward += rejected_reward.item()\n",
    "            total_bt_prob += bt_prob\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                train_iter.set_postfix({\"loss\": loss.item(), \"BT_prob\": bt_prob})\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_chosen = total_chosen_reward / len(train_loader)\n",
    "        avg_rejected = total_rejected_reward / len(train_loader)\n",
    "        avg_bt_prob = total_bt_prob / len(train_loader)\n",
    "\n",
    "        print(f'Epoch {epoch}: Loss={avg_loss:.4f}, '\n",
    "              f'Chosen={avg_chosen:.4f}, Rejected={avg_rejected:.4f}, '\n",
    "              f'BT_Prob={avg_bt_prob:.4f}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_chosen_reward = 0\n",
    "        val_rejected_reward = 0\n",
    "        val_bt_prob = 0\n",
    "\n",
    "        val_iter = tqdm(val_dataloader, total=len(val_dataloader), desc=f\"Epoch {epoch} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                loss, chosen_reward, rejected_reward = compute_loss(model, batch)\n",
    "                bt_prob = torch.sigmoid(chosen_reward - rejected_reward).item()\n",
    "                val_loss += loss.item()\n",
    "                val_chosen_reward += chosen_reward.item()\n",
    "                val_rejected_reward += rejected_reward.item()\n",
    "                val_bt_prob += bt_prob\n",
    "                val_iter.set_postfix({\"val_loss\": loss.item(), \"val_BT_prob\": bt_prob})\n",
    "\n",
    "        val_avg_loss = val_loss / len(val_dataloader)\n",
    "        val_avg_chosen = val_chosen_reward / len(val_dataloader)\n",
    "        val_avg_rejected = val_rejected_reward / len(val_dataloader)\n",
    "        val_avg_bt_prob = val_bt_prob / len(val_dataloader)\n",
    "\n",
    "        print(f'Validation: Loss={val_avg_loss:.4f}, '\n",
    "              f'Chosen={val_avg_chosen:.4f}, Rejected={val_avg_rejected:.4f}, '\n",
    "              f'BT_Prob={val_avg_bt_prob:.4f}')\n",
    "\n",
    "        # Test model on sample context\n",
    "        if start_context:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                start_tokens = tokenizer.encode(start_context)\n",
    "                start_tensor = torch.tensor(start_tokens).unsqueeze(0).to(device)\n",
    "                feedback = model(start_tensor)\n",
    "                print(f\"Sample reward: {feedback[0, -1, 0].item():.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_reward_model(base_model, base_config, device):\n",
    "    \"\"\"Setup reward model from pre-trained base model\"\"\"\n",
    "    \n",
    "    # Freeze all parameters first\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace output head with reward head (single scalar output)\n",
    "    num_classes = 1\n",
    "    base_model.out_head = torch.nn.Linear(\n",
    "        in_features=base_config[\"emb_dim\"], \n",
    "        out_features=num_classes\n",
    "    )\n",
    "    \n",
    "    # Unfreeze last transformer block for fine-tuning\n",
    "    for param in base_model.trf_blocks[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Unfreeze final normalization layer\n",
    "    for param in base_model.final_norm.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Unfreeze the new output head\n",
    "    for param in base_model.out_head.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    base_model.to(device)\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_path = \"gpt_models\\\\instruct-GPT2-355M-SFT.pth\"\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=True\n",
    "    )['model_state_dict']\n",
    ")\n",
    "model = setup_reward_model(base_model=model, base_config=BASE_CONFIG, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "start_context = tes_data[0][\"chosen\"]\n",
    "epochs = 20\n",
    "batch_size = 10\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [Train]: 100%|██████████| 85/85 [08:33<00:00,  6.04s/it, loss=0.486, BT_prob=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=0.7621, Chosen=0.3019, Rejected=0.2826, BT_Prob=0.5048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.09s/it, val_loss=0.681, val_BT_prob=0.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=0.8587, Chosen=0.0524, Rejected=0.2787, BT_Prob=0.4443\n",
      "Sample reward: 0.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 85/85 [08:25<00:00,  5.94s/it, loss=0.662, BT_prob=0.54] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=0.6717, Chosen=0.2282, Rejected=0.0585, BT_Prob=0.5415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=0.711, val_BT_prob=0.512]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=0.8476, Chosen=0.0457, Rejected=0.2510, BT_Prob=0.4493\n",
      "Sample reward: 0.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 85/85 [08:18<00:00,  5.86s/it, loss=0.551, BT_prob=0.612]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=0.5820, Chosen=0.3109, Rejected=-0.0765, BT_Prob=0.5944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.10s/it, val_loss=0.724, val_BT_prob=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=0.8306, Chosen=0.1994, Rejected=0.3487, BT_Prob=0.4631\n",
      "Sample reward: 0.3910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 85/85 [08:11<00:00,  5.78s/it, loss=0.532, BT_prob=0.649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=0.4744, Chosen=0.5864, Rejected=-0.1480, BT_Prob=0.6730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it, val_loss=0.779, val_BT_prob=0.487]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=0.8791, Chosen=0.3001, Rejected=0.4733, BT_Prob=0.4573\n",
      "Sample reward: 0.7441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 85/85 [08:16<00:00,  5.84s/it, loss=0.24, BT_prob=0.821] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=0.3687, Chosen=0.8117, Rejected=-0.3566, BT_Prob=0.7567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.10s/it, val_loss=0.763, val_BT_prob=0.494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=0.9076, Chosen=0.2356, Rejected=0.3829, BT_Prob=0.4639\n",
      "Sample reward: 1.1603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 85/85 [08:23<00:00,  5.92s/it, loss=0.326, BT_prob=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=0.2804, Chosen=1.0718, Rejected=-0.6096, BT_Prob=0.8368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=0.845, val_BT_prob=0.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=0.9825, Chosen=0.4267, Rejected=0.5807, BT_Prob=0.4633\n",
      "Sample reward: 1.7561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 85/85 [08:37<00:00,  6.09s/it, loss=0.233, BT_prob=0.93] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=0.2160, Chosen=1.2842, Rejected=-1.0305, BT_Prob=0.9008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=0.954, val_BT_prob=0.431]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.0543, Chosen=0.7271, Rejected=0.8331, BT_Prob=0.4759\n",
      "Sample reward: 2.4781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 85/85 [08:50<00:00,  6.24s/it, loss=0.269, BT_prob=0.93]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=0.1727, Chosen=1.8201, Rejected=-1.2675, BT_Prob=0.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=0.875, val_BT_prob=0.483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.1414, Chosen=0.3699, Rejected=0.4583, BT_Prob=0.4808\n",
      "Sample reward: 2.0601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 85/85 [08:35<00:00,  6.06s/it, loss=0.149, BT_prob=0.976] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=0.1484, Chosen=2.1939, Rejected=-1.6984, BT_Prob=0.9732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it, val_loss=0.917, val_BT_prob=0.484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.2358, Chosen=0.6818, Rejected=0.7506, BT_Prob=0.4873\n",
      "Sample reward: 2.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 85/85 [08:28<00:00,  5.98s/it, loss=0.0726, BT_prob=0.995]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=0.1363, Chosen=2.4275, Rejected=-2.2135, BT_Prob=0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it, val_loss=0.969, val_BT_prob=0.466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.3025, Chosen=0.5825, Rejected=0.6022, BT_Prob=0.4989\n",
      "Sample reward: 2.4487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|██████████| 85/85 [08:43<00:00,  6.16s/it, loss=0.0708, BT_prob=0.997] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=0.1315, Chosen=2.5865, Rejected=-2.5667, BT_Prob=0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=0.999, val_BT_prob=0.463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.3497, Chosen=0.4353, Rejected=0.4329, BT_Prob=0.5046\n",
      "Sample reward: 2.4292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Train]: 100%|██████████| 85/85 [08:23<00:00,  5.92s/it, loss=0.139, BT_prob=0.998] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=0.1300, Chosen=2.6636, Rejected=-2.8663, BT_Prob=0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it, val_loss=1.02, val_BT_prob=0.462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.3982, Chosen=0.4481, Rejected=0.4188, BT_Prob=0.5111\n",
      "Sample reward: 2.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Train]: 100%|██████████| 85/85 [08:42<00:00,  6.15s/it, loss=0.00154, BT_prob=0.999]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=0.1288, Chosen=2.8099, Rejected=-3.0371, BT_Prob=0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=1.04, val_BT_prob=0.454] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.4293, Chosen=0.5051, Rejected=0.4546, BT_Prob=0.5155\n",
      "Sample reward: 2.5939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Train]: 100%|██████████| 85/85 [08:25<00:00,  5.95s/it, loss=0.14, BT_prob=0.997]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=0.1283, Chosen=2.9789, Rejected=-3.1109, BT_Prob=0.9965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=1.05, val_BT_prob=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.4583, Chosen=0.5206, Rejected=0.4538, BT_Prob=0.5190\n",
      "Sample reward: 2.6190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Train]: 100%|██████████| 85/85 [08:33<00:00,  6.04s/it, loss=0.209, BT_prob=0.995]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=0.1279, Chosen=3.0600, Rejected=-3.2501, BT_Prob=0.9968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it, val_loss=1.06, val_BT_prob=0.451]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.4846, Chosen=0.5201, Rejected=0.4399, BT_Prob=0.5214\n",
      "Sample reward: 2.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Train]: 100%|██████████| 85/85 [08:37<00:00,  6.09s/it, loss=0.347, BT_prob=0.982] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=0.1278, Chosen=3.1557, Rejected=-3.3393, BT_Prob=0.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.10s/it, val_loss=1.07, val_BT_prob=0.452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.5129, Chosen=0.5711, Rejected=0.4836, BT_Prob=0.5232\n",
      "Sample reward: 2.7057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Train]: 100%|██████████| 85/85 [08:36<00:00,  6.08s/it, loss=0.139, BT_prob=0.998] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss=0.1277, Chosen=3.2278, Rejected=-3.4362, BT_Prob=0.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=1.08, val_BT_prob=0.448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.5357, Chosen=0.5223, Rejected=0.4258, BT_Prob=0.5248\n",
      "Sample reward: 2.6612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Train]: 100%|██████████| 85/85 [08:38<00:00,  6.11s/it, loss=0.208, BT_prob=0.997] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=0.1276, Chosen=3.2523, Rejected=-3.5706, BT_Prob=0.9980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=1.1, val_BT_prob=0.441] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.5571, Chosen=0.4806, Rejected=0.3771, BT_Prob=0.5257\n",
      "Sample reward: 2.6792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Train]: 100%|██████████| 85/85 [08:20<00:00,  5.89s/it, loss=0.0696, BT_prob=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss=0.1275, Chosen=3.2808, Rejected=-3.6829, BT_Prob=0.9981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.11s/it, val_loss=1.11, val_BT_prob=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.5751, Chosen=0.4884, Rejected=0.3746, BT_Prob=0.5277\n",
      "Sample reward: 2.7087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Train]: 100%|██████████| 85/85 [08:16<00:00,  5.84s/it, loss=0.347, BT_prob=0.985] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss=0.1275, Chosen=3.3835, Rejected=-3.7153, BT_Prob=0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Val]: 100%|██████████| 5/5 [00:20<00:00,  4.12s/it, val_loss=1.12, val_BT_prob=0.438]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss=1.5912, Chosen=0.4863, Rejected=0.3613, BT_Prob=0.5296\n",
      "Sample reward: 2.7306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reward_model = train_reward_model(model,\n",
    "    optimizer,\n",
    "    device,\n",
    "    train_data,\n",
    "    val_data,\n",
    "    tokenizer,\n",
    "    start_context,\n",
    "    epochs,\n",
    "    batch_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model has been saved successfully at d:\\software_3\\Generative_models\\Text_models\\chat_gpt2\\gpt_models\\RLHF_reward_model.pth\n"
     ]
    }
   ],
   "source": [
    "save_model(reward_model, \"RLHF_reward_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_path = \"gpt_models\\\\RLHF_reward_model.pth\"\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=True\n",
    "    )\n",
    ")\n",
    "#model = setup_reward_model(base_model=model, base_config=BASE_CONFIG, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(model, tokenizer, prompt, response, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        start_tokens = f\"{prompt} \\n\\nResponse: {response}\"\n",
    "        start_tokens = tokenizer.encode(start_tokens)\n",
    "        start_tensor = torch.tensor(start_tokens).unsqueeze(0).to(device)\n",
    "        reward = model(start_tensor)\n",
    "        # reward shape: [1, seq_len, 1], get the last token's reward\n",
    "        return reward[0, -1, 0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1 score: -6.01485538482666\n",
      "Response 2 score: -4.657999515533447\n",
      "P(Response 2 > Response 1) = 0.7952\n",
      "Chosen response has higher score: True\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Is having your medical records online safe?\"\n",
    "response1 = \"You mean being able to share them with your doctor, or making them public to the internet in general?\"\n",
    "response2 = \"Hm, I think so! If you have your records online, it makes it easier for you to access them, and also for your doctor to share updates with you, and for you to share updates with your doctor. It also makes it easier for you to share your information with other people, if you want, for example to join a health advocacy or support group. The only risk I see is that if someone else has access to your records, they might abuse that access, or they might accidentally make a change to your records that you don\\u2019t want. But overall, I think online access is a good thing.\"\n",
    "    \n",
    "score1 = evaluate_response(model, tokenizer, prompt, response1, device)\n",
    "score2 = evaluate_response(model, tokenizer, prompt, response2, device)\n",
    "    \n",
    "    # Compute Bradley-Terry probability\n",
    "bt_prob = torch.sigmoid(torch.tensor(score2-score1)).item()\n",
    "    \n",
    "print(f\"Response 1 score: {score1}\")\n",
    "print(f\"Response 2 score: {score2}\")\n",
    "print(f\"P(Response 2 > Response 1) = {bt_prob:.4f}\")\n",
    "print(f\"Chosen response has higher score: {score2 > score1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
