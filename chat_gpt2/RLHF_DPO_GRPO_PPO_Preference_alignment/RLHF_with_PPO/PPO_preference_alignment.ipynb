{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1185bec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2\\\\RLHF_DPO_Preference_alignment\\\\RLHF_with_PPO'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2e8c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79d872a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdabe07",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9498421",
   "metadata": {},
   "source": [
    "This Notebook implements the PPO RLHF from this [Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment](https://arxiv.org/abs/2310.00212) research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from gpt import GPTModel\n",
    "from model_args import BASE_CONFIG\n",
    "from utils.generate import generate\n",
    "from utils.token_converter import get_tokenizer, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14afbc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"oasst1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Now you can access the data as a list of dictionaries\n",
    "print(data[0][\"prompt\"])  # Prints the prompt of the first object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc5ef9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately complates the request.\"\n",
    "        f\"\\n\\n## Instruction:\\n{entry[\"prompt\"]}\"\n",
    "    )\n",
    "    return instruction_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b96ac880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPODataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        self.encoded_text = []\n",
    "        for entry in data:\n",
    "            prompt = format_input(entry)\n",
    "\n",
    "            prompt_tokens = tokenizer.encode(prompt)\n",
    "\n",
    "            self.encoded_text.append(prompt_tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_text[index]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927e0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fun(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    allowed_max_length=None,\n",
    "    mask_prompt_tokens=True,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    batch_data = {\n",
    "        \"prompt\": []\n",
    "    }\n",
    "\n",
    "    max_length_comman = 0\n",
    "    if batch:\n",
    "        current_max_length = max(len(item) + 1 for item in batch)  # item is a list\n",
    "        max_length_comman = max(max_length_comman, current_max_length)\n",
    "\n",
    "    for item in batch:\n",
    "        prompt = torch.tensor(item)\n",
    "        batch_data[\"prompt\"].append(prompt)\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    padded_prompts = nn.utils.rnn.pad_sequence(batch_data[\"prompt\"], batch_first=True, padding_value=pad_token_id)\n",
    "    if allowed_max_length is not None:\n",
    "        padded_prompts = padded_prompts[:, :allowed_max_length]\n",
    "\n",
    "    batch_data[\"prompt\"] = padded_prompts.to(device)\n",
    "\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "482a4ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fun,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a295b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "451af8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "\n",
    "example_dataset = PPODataset(example_data, tokenizer)\n",
    "\n",
    "example_dataloader = DataLoader(\n",
    "    example_dataset,\n",
    "    batch_size=2,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "286e648f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.keys: dict_keys(['prompt'])\n"
     ]
    }
   ],
   "source": [
    "for batch in example_dataloader:\n",
    "    break\n",
    "\n",
    "print(\"batch.keys:\", batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87fa5941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431,  2299,   689,   262,  2581,    13,   198,\n",
       "           198,  2235, 46486,    25,   198,  6090,   345,  3551,   257,  1790,\n",
       "          9793,   546,   262, 23082,   286,   262,  3381,   366,  2144,   404,\n",
       "          1559,    88,     1,   287, 12446,    30,  4222,   779,  6096,  3519,\n",
       "           284,  2785, 15848,  1559,   444,   287,   262, 10515,  1910,   290,\n",
       "         21729,  5981,  2267,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256],\n",
       "        [21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n",
       "           257,  2882,   326, 20431,  2299,   689,   262,  2581,    13,   198,\n",
       "           198,  2235, 46486,    25,   198,     1,  9069,   404,  1559,    88,\n",
       "             1, 10229,   284,   257,  1910,  4645,   810,   612,   318,   691,\n",
       "           530, 17872,   329,   257,  1948,   922,   393,  2139,    13,   554,\n",
       "         12446,    11,   428,  3381,   318,  3573,  5981,   287,   262,  4827,\n",
       "          1910,    11,   810,   257, 15848,  1559,    88,  9749,   468,  2383,\n",
       "          1176,   625,   262,  9400,   290,  1762,  3403,   286,   511,  4409,\n",
       "            13,   383,  4931,   286,   257, 15848,  1559,    88,   460,  1255,\n",
       "           287,  2793,  9400,   290,  5322,  7184,  6443,   329,  3259,    11,\n",
       "           355,   262,  9749,   468,  1310, 15660,   284,  2620,  9400,   393,\n",
       "          2148,  1365,  1762,  3403,    13,   198,   198, 26446,  2267,   468,\n",
       "          5174,  2785, 15848,  1559,   444,   287, 11798,   884,   355,  6308,\n",
       "           290,  3049,  2057,    11,   810,   257,  1178,  1588,  2706,  1630,\n",
       "           257,  2383,  6903,   286,   262,  1910,   357,    33,   452,   641,\n",
       "          1222, 14136,  2978,    11,  2211,   737,   554,   777, 11798,    11,\n",
       "          3259,  1690,  1986,  1877,  9400,    11,  3614,  4034,    11,   290,\n",
       "          5322, 23189,  1176,    11,  3756,   284,   257,  3074,   810,   484,\n",
       "           389, 10795,   319,   262,  9749,   329,   511, 30489,    13,   770,\n",
       "         21403,   460,  1255,   287,  2252, 22711,   286,  9400,   290,   257,\n",
       "          7794,   287,  1762,  3403,    13,   198,   198, 16350,    11,   262,\n",
       "          3721,   286, 15848,  1559,    88,   318,  6393,   284,  4547,   262,\n",
       "         17262,   286,  4827,  5939,   290,   262,  2928,   286,  1910,  1176,\n",
       "           319,  3259,    13,  7735,  2267,   318,  2622,   284,  1833,   262,\n",
       "          6287,   290,  2928,   286, 15848,  1559,   444,   319,   262,  3773,\n",
       "           290,   284,  1205,  4788,   284,  2209,   428,  2071,    13,   198,\n",
       "           198, 19927,    25,   198,    33,   452,   641,    11,   449,  1539,\n",
       "          1222, 14136,  2978,    11,   406,    13,   357,  6390,   737,   383,\n",
       "          7119,   286, 26040,  8393,   315,  1083,   290, 11302, 43793,   874,\n",
       "           355, 21259,   286,   371,   658,   287,  5849,   352, 22512,   554,\n",
       "          8988,    13,  4913,   286, 11279, 29845,  1083,    11,  2681,     7,\n",
       "            18,   828,  7632,    12,  3695,    13]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da00cf87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately complates the request.\\n\\n## Instruction:\\nCan you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids_to_text(batch[\"prompt\"][0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c14d90d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 4250\n",
      "Validation set length: 250\n",
      "Test set length: 500\n"
     ]
    }
   ],
   "source": [
    "def split_data(data):\n",
    "    train_portion = int(len(data) * 0.85)\n",
    "    test_portion = int(len(data) * 0.1)\n",
    "    val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "    train_data = data[:train_portion]\n",
    "    test_data = data[train_portion:train_portion + test_portion]\n",
    "    val_data = data[train_portion + test_portion:]\n",
    "    \n",
    "    print(\"Training set length:\", len(train_data))\n",
    "    print(\"Validation set length:\", len(val_data))\n",
    "    print(\"Test set length:\", len(test_data))\n",
    "\n",
    "    return train_data, test_data, val_data\n",
    "\n",
    "train_data, test_data, val_data = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dfd41b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "train_dataset = PPODataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers)\n",
    "\n",
    "test_dataset = PPODataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = PPODataset(val_data, tokenizer)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f352118e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"gpt_models\\\\SFT_model.pth\"\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=True\n",
    "    )['model_state_dict']\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4beb3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response\n",
    "that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26165680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\n",
      "\n",
      "### Input:\n",
      "Generate a sentence using the word \"economics\". \n",
      "\n",
      "### Response:\n",
      "The researchers had a dynamic economy that led to the success\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(data[0][\"prompt\"], tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "\n",
    "response = token_ids_to_text(token_ids, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aff52261",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = model\n",
    "\n",
    "reference_model = GPTModel(BASE_CONFIG)\n",
    "reference_model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=True\n",
    "    )['model_state_dict']\n",
    ")\n",
    "reference_model.eval()\n",
    "\n",
    "policy_model.to(device)\n",
    "reference_model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f48b36cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model = GPTModel(BASE_CONFIG)\n",
    "num_classes = 1\n",
    "reward_model.out_head = torch.nn.Linear(\n",
    "    in_features=BASE_CONFIG[\"emb_dim\"], \n",
    "    out_features=num_classes\n",
    ")\n",
    "model_path = \"gpt_models\\\\RLHF_reward_model.pth\"\n",
    "reward_model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=True\n",
    "    )\n",
    ")\n",
    "reward_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89e30e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_value_model(base_model, base_config, device):\n",
    "    \"\"\"Setup reward model from pre-trained base model\"\"\"\n",
    "    \n",
    "    # Freeze all parameters first\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Replace output head with reward head (single scalar output)\n",
    "    num_classes = 1\n",
    "    base_model.out_head = torch.nn.Linear(\n",
    "        in_features=base_config[\"emb_dim\"], \n",
    "        out_features=num_classes\n",
    "    )\n",
    "    \n",
    "    # Unfreeze last transformer block for fine-tuning\n",
    "    for param in base_model.trf_blocks[-1].parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Unfreeze final normalization layer\n",
    "    for param in base_model.final_norm.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # Unfreeze the new output head\n",
    "    for param in base_model.out_head.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    base_model.to(device)\n",
    "    return base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3313f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"gpt_models\\\\SFT_model.pth\"\n",
    "value_model = GPTModel(BASE_CONFIG)\n",
    "value_model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=True\n",
    "    )['model_state_dict']\n",
    ")\n",
    "value_model = setup_value_model(base_model=value_model, base_config=BASE_CONFIG, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca79ec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(value_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71282a1",
   "metadata": {},
   "source": [
    "## PPO Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571fc28c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54d9ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_max_context(input_ids, max_context=1024):\n",
    "    if input_ids.shape[1] > max_context:\n",
    "        input_ids = input_ids[:, -max_context:]\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ff4f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveKLController:\n",
    "\n",
    "    def __init__(self, kl_coef=0.01, target_kl=6.0):\n",
    "        self.kl_coef = kl_coef\n",
    "        self.target = target_kl\n",
    "\n",
    "    def update(self, current_kl, n_steps):\n",
    "        proportional_error = np.clip(current_kl / self.target -1, -0.2, 0.2)\n",
    "        mult = 1 + proportional_error * n_steps / 20\n",
    "        self.kl_coef *= mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeced12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_logprobs(sequence, scores):\n",
    "    logprobs = []\n",
    "\n",
    "    for i, score in enumerate(scores):\n",
    "        if i < len(sequence[0]) -1:\n",
    "            next_token = sequence[0][i+1]\n",
    "            logprob = F.log_softmax(score, dim= -1)[0, next_token]\n",
    "            logprobs.append(logprob)\n",
    "    return torch.stack(logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "768708d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_rewards(reward_model, responses, tokenizer, config, device=None):\n",
    "    \"\"\"\n",
    "    Computes scalar rewards for each response using the reward model.\n",
    "\n",
    "    Args:\n",
    "        reward_model: The reward model (should output a tensor per input).\n",
    "        responses: List of response strings.\n",
    "        tokenizer: Tokenizer to convert text to token ids.\n",
    "        config: PPOConfig object (for normalization/scaling).\n",
    "        device: torch.device (optional, if not provided, uses reward_model's device).\n",
    "\n",
    "    Returns:\n",
    "        rewards: Tensor of shape [batch_size, 1] (one reward per response).\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = next(reward_model.parameters()).device\n",
    "    \n",
    "    rewards = []\n",
    "    with torch.no_grad():\n",
    "        for response in responses:\n",
    "            # Convert response to token ids\n",
    "            input_ids = text_to_token_ids(response, tokenizer)\n",
    "            if isinstance(input_ids, list):\n",
    "                input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "            if input_ids.ndim == 1:\n",
    "                input_ids = input_ids.unsqueeze(0)  # [1, seq_len]\n",
    "            input_ids = input_ids.to(device)\n",
    "\n",
    "            # Get reward from model\n",
    "            input_ids = ensure_max_context(input_ids)\n",
    "            reward = reward_model(input_ids)\n",
    "            # If reward is [1, seq_len, 1] or [1, seq_len], take last token's reward\n",
    "            if reward.ndim > 1:\n",
    "                reward = reward[0, -1]\n",
    "            # Ensure reward is a scalar tensor\n",
    "            reward = reward.unsqueeze(0)  # [1]\n",
    "            rewards.append(reward)\n",
    "\n",
    "    rewards = torch.cat(rewards, dim=0)  # [batch_size, 1]\n",
    "\n",
    "    # Optional: normalize and/or clip rewards\n",
    "    if getattr(config, \"use_score_norm\", False):\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)\n",
    "    if getattr(config, \"use_score_scaling\", False):\n",
    "        rewards = torch.clamp(rewards, -config.score_clip, config.score_clip)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19eb1f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards, values, config):\n",
    "    advantages = []\n",
    "    returns = []\n",
    "\n",
    "    rewards_np = rewards.detach().cpu().numpy()\n",
    "    values_np = values.detach().cpu().numpy()\n",
    "\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards_np))):\n",
    "        if i == len(rewards_np) -1:\n",
    "            next_value = 0\n",
    "        else:\n",
    "            next_value = values_np[i + 1]\n",
    "\n",
    "        delta = rewards_np[i] + config.gamma * next_value - values_np[i]\n",
    "        gae = delta + config.gamma * config.lam * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    advantages = torch.tensor(advantages, device=device)\n",
    "    returns = advantages + values\n",
    "\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    return advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33362f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(prompts, policy_model, value_model, tokenizer):\n",
    "    policy_model.eval()\n",
    "    MAX_CONTEXT = 1024\n",
    "    responses = []\n",
    "    all_logprobs = []\n",
    "    all_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            prompt_text = token_ids_to_text(prompt, tokenizer)\n",
    "            input_ids = text_to_token_ids(prompt_text, tokenizer)\n",
    "            if isinstance(input_ids, list):\n",
    "                input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "            if input_ids.ndim == 1:\n",
    "                input_ids = input_ids.unsqueeze(0)\n",
    "            input_ids = ensure_max_context(input_ids, MAX_CONTEXT)\n",
    "            input_ids = input_ids.to(policy_model.tok_emb.weight.device)\n",
    "\n",
    "            max_new_tokens = min(100, MAX_CONTEXT - input_ids.shape[1])\n",
    "            output = generate(\n",
    "                model=policy_model,\n",
    "                idx=input_ids,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                context_size=MAX_CONTEXT,\n",
    "                eos_id=50256\n",
    "            )\n",
    "            generated_ids = output[0]\n",
    "            if generated_ids.ndim == 1:\n",
    "                generated_ids = generated_ids.unsqueeze(0)\n",
    "            generated_ids = ensure_max_context(generated_ids, MAX_CONTEXT)\n",
    "            response = token_ids_to_text(generated_ids[0], tokenizer)\n",
    "            responses.append(response)\n",
    "\n",
    "            logits = policy_model(generated_ids)\n",
    "            if logits.ndim == 2:\n",
    "                logits = logits.unsqueeze(0)\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = generated_ids[..., 1:].contiguous()\n",
    "            logprobs_tensor = F.log_softmax(shift_logits, dim=-1)\n",
    "            logprobs_for_labels = logprobs_tensor.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "            all_logprobs.append(logprobs_for_labels.mean())\n",
    "\n",
    "            values = value_model(generated_ids)\n",
    "            if values.ndim > 1:\n",
    "                values = values[0, -1]\n",
    "            all_values.append(values)\n",
    "\n",
    "    return responses, torch.stack(all_logprobs), torch.stack(all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b6e6e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(responses, policy_model, value_model, tokenizer):\n",
    "    logprobs = []\n",
    "    values = []\n",
    "    for response in responses:\n",
    "        input_ids = text_to_token_ids(response, tokenizer)\n",
    "        if isinstance(input_ids, list):\n",
    "            input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        if input_ids.ndim == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)  # [1, seq_len]\n",
    "        input_ids = input_ids.to(next(policy_model.parameters()).device)\n",
    "\n",
    "        logits = policy_model(input_ids)  # [batch, seq_len, vocab_size] or [seq_len, vocab_size]\n",
    "        if logits.ndim == 2:\n",
    "            logits = logits.unsqueeze(0)  # [1, seq_len, vocab_size]\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = input_ids[..., 1:].contiguous()\n",
    "\n",
    "        import torch.nn.functional as F\n",
    "        logprobs_tensor = F.log_softmax(shift_logits, dim=-1)\n",
    "        # Gather logprobs for the actual next tokens\n",
    "        logprobs_for_labels = logprobs_tensor.gather(2, shift_labels.unsqueeze(-1)).squeeze(-1)\n",
    "        # Optionally, mean over tokens\n",
    "        logprobs.append(logprobs_for_labels.mean())\n",
    "\n",
    "        value = value_model(input_ids)\n",
    "        if value.ndim > 1:\n",
    "            value = value[0, -1]\n",
    "        values.append(value)\n",
    "\n",
    "    return torch.stack(logprobs), torch.stack(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d435db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(policy_model,\n",
    "                value_model,\n",
    "                policy_optimizer,\n",
    "                value_optimizer,\n",
    "                kl_ctl,\n",
    "                stats,\n",
    "                tokenizer,\n",
    "                responses,\n",
    "                old_logprobs,\n",
    "                rewards,\n",
    "                advantages,\n",
    "                returns,\n",
    "                config):\n",
    "\n",
    "    batch_size = len(responses)\n",
    "    indices = torch.randperm(batch_size)\n",
    "\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    kl_divergence = []\n",
    "\n",
    "    for epoch in range(config.ppo_epochs):\n",
    "        for i in range(0, batch_size, config.mini_batch_size):\n",
    "            mini_batch_indices = indices[i:i + config.mini_batch_size]\n",
    "\n",
    "            mini_responses = [responses[idx] for idx in mini_batch_indices]\n",
    "            mini_old_logprobs = old_logprobs[mini_batch_indices]\n",
    "            mini_advantages = advantages[mini_batch_indices]\n",
    "            mini_returns = returns[mini_batch_indices]\n",
    "\n",
    "            policy_model.train()\n",
    "            value_model.train()\n",
    "\n",
    "            new_logprobs, new_values = forward_pass(mini_responses, policy_model, value_model, tokenizer)\n",
    "            ratio = torch.exp(new_logprobs - mini_old_logprobs)\n",
    "            surr1 = ratio * mini_advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range) * mini_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            value_loss = F.mse_loss(new_values, mini_returns)\n",
    "\n",
    "            kl_div = (mini_old_logprobs - new_logprobs).mean()\n",
    "\n",
    "            total_loss = policy_loss + config.vf_coef * value_loss + config.kl_coef * kl_div\n",
    "\n",
    "            policy_optimizer.zero_grad()\n",
    "            value_optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(policy_model.parameters(), config.max_grad_norm)\n",
    "            torch.nn.utils.clip_grad_norm_(value_model.parameters(), config.max_grad_norm)\n",
    "\n",
    "            policy_losses.append(policy_loss.item())\n",
    "            value_losses.append(value_loss.item())\n",
    "            kl_divergence.append(kl_div.item())\n",
    "\n",
    "    mean_kl = np.mean(kl_divergence)\n",
    "    kl_ctl.update(mean_kl, batch_size)\n",
    "\n",
    "    stats['policy_loss'].extend(policy_losses)\n",
    "    stats['value_loss'].extend(value_losses)\n",
    "    stats['kl_divergence'].extend(kl_divergence)\n",
    "    stats['rewards'].extend(rewards.tolist())\n",
    "    stats['advantages'].extend(advantages.tolist())\n",
    "\n",
    "    return {\n",
    "        'policy_loss': np.mean(policy_losses),\n",
    "        'value_loss': np.mean(value_losses),\n",
    "        'kl_divergence': mean_kl,\n",
    "        'mean_reward': rewards.mean().item(),\n",
    "        'mean_advantage': advantages.mean().item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1e4dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    batch,\n",
    "    policy_model,\n",
    "    reward_model,\n",
    "    value_model,\n",
    "    policy_optimizer,\n",
    "    value_optimizer,\n",
    "    kl_ctl,\n",
    "    stats,\n",
    "    tokenizer,\n",
    "    config\n",
    "\n",
    "):\n",
    "\n",
    "    prompts = batch['prompt']\n",
    "    responses, old_logprobs, old_values = generate_responses(prompts, policy_model, value_model, tokenizer)\n",
    "    rewards = compute_rewards(reward_model, responses, tokenizer, config)\n",
    "    advantages, returns = compute_advantages(rewards, old_values, config)\n",
    "    metrics = ppo_update(policy_model,\n",
    "                value_model,\n",
    "                policy_optimizer,\n",
    "                value_optimizer,\n",
    "                kl_ctl,\n",
    "                stats,\n",
    "                tokenizer,\n",
    "                responses,\n",
    "                old_logprobs,\n",
    "                rewards,\n",
    "                advantages,\n",
    "                returns,\n",
    "                config)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec580f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class PPOTrainer:\n",
    "\n",
    "    def __init__(self,\n",
    "                config,\n",
    "                policy_model,\n",
    "                ref_model,\n",
    "                reward_model,\n",
    "                value_model,\n",
    "                tokenizer):\n",
    "\n",
    "        self.config = config\n",
    "        self.policy_model = policy_model\n",
    "        self.ref_model = ref_model\n",
    "        self.reward_model = reward_model\n",
    "        self.value_model = value_model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        for param in self.ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(\n",
    "            self.policy_model.parameters(), lr=self.config.learning_rate\n",
    "        )\n",
    "\n",
    "        self.value_optimizer = torch.optim.Adam(\n",
    "            self.value_model.parameters(), lr=self.config.learning_rate\n",
    "        )\n",
    "\n",
    "        self.kl_ctl = AdaptiveKLController(config.kl_coef, config.kl_target)\n",
    "\n",
    "        self.stats = {\n",
    "                    'policy_loss': [],\n",
    "                    'value_loss': [],\n",
    "                    'kl_divergence': [],\n",
    "                    'rewards': [],\n",
    "                    'advantages': []\n",
    "                    }\n",
    "\n",
    "    def train(self, dataloader, num_epochs=1):\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_metrics = []\n",
    "\n",
    "            for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}\")):\n",
    "                metrics = train_step(\n",
    "                                        batch,\n",
    "                                        self.policy_model,\n",
    "                                        self.reward_model,\n",
    "                                        self.value_model,\n",
    "                                        self.policy_optimizer,\n",
    "                                        self.value_optimizer,\n",
    "                                        self.kl_ctl,\n",
    "                                        self.stats,\n",
    "                                        self.tokenizer,\n",
    "                                        self.config\n",
    "                                    )\n",
    "                epoch_metrics.append(metrics)\n",
    "\n",
    "                if batch_idx % 10 == 0:\n",
    "                    avg_metrics = {k: np.mean([m[k] for m in epoch_metrics[-10:]]) for k in metrics.keys()}\n",
    "                    print(f\"Batch {batch_idx}: {avg_metrics}\")\n",
    "            epoch_avg = {k: np.mean([m[k] for m in epoch_metrics]) for k in epoch_metrics[0].keys()}\n",
    "            print(f\"Epoch {epoch+1} Summary: {epoch_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "569acac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PPOConfig:\n",
    "    \"\"\"Configuration for PPO training\"\"\"\n",
    "    model_name: str = \"gpt2\"\n",
    "    learning_rate: float = 1e-6\n",
    "    batch_size: int = 8\n",
    "    mini_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    ppo_epochs: int = 4\n",
    "    max_grad_norm: float = 1.0\n",
    "    clip_range: float = 0.2\n",
    "    clip_range_vf: float = 0.2\n",
    "    vf_coef: float = 0.1\n",
    "    kl_coef: float = 0.01\n",
    "    target_kl: float = 0.1\n",
    "    max_length: int = 512\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 50\n",
    "    top_p: float = 0.95\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.95\n",
    "    entropy_coef: float = 0.01\n",
    "    adaptive_kl: bool = True\n",
    "    kl_target: float = 6.0\n",
    "    horizon: int = 10000\n",
    "    use_score_scaling: bool = True\n",
    "    use_score_norm: bool = True\n",
    "    score_clip: float = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b1bb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    learning_rate=1e-6,\n",
    "    batch_size=8,\n",
    "    ppo_epochs=4,\n",
    "    max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "28458e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6119244",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(\n",
    "                    config,\n",
    "                    policy_model,\n",
    "                    reference_model,\n",
    "                    reward_model,\n",
    "                    value_model,\n",
    "                    tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07304a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/531 [00:00<?, ?it/s]C:\\Users\\rppaw\\AppData\\Local\\Temp\\ipykernel_15452\\3974377106.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  advantages = torch.tensor(advantages, device=device)\n",
      "Epoch 1:   0%|          | 1/531 [01:48<15:58:27, 108.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: {'policy_loss': np.float64(0.20140691008418798), 'value_loss': np.float64(1.1066711321473122), 'kl_divergence': np.float64(1.5953607559204102), 'mean_reward': np.float64(0.0), 'mean_advantage': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   2%|         | 11/531 [12:06<8:19:16, 57.61s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10: {'policy_loss': np.float64(0.11008045729249716), 'value_loss': np.float64(3.131737431138754), 'kl_divergence': np.float64(1.0589583575725556), 'mean_reward': np.float64(-1.9371509552001954e-08), 'mean_advantage': np.float64(-2.2351741790771484e-09)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|         | 21/531 [28:36<14:53:41, 105.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20: {'policy_loss': np.float64(0.11892779916524887), 'value_loss': np.float64(3.6807915715500714), 'kl_divergence': np.float64(0.9123098909854889), 'mean_reward': np.float64(2.2351741790771484e-09), 'mean_advantage': np.float64(-1.1175870895385742e-09)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   6%|         | 31/531 [53:47<34:42:02, 249.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 30: {'policy_loss': np.float64(0.07020856142044067), 'value_loss': np.float64(3.4865598507225513), 'kl_divergence': np.float64(0.8262212872505188), 'mean_reward': np.float64(-1.2665987014770507e-08), 'mean_advantage': np.float64(1.3411045074462891e-08)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   8%|         | 41/531 [1:16:17<20:46:39, 152.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40: {'policy_loss': np.float64(0.10168765243142844), 'value_loss': np.float64(2.5973120304523034), 'kl_divergence': np.float64(0.7822708308696746), 'mean_reward': np.float64(5.140900611877442e-08), 'mean_advantage': np.float64(-8.568167686462402e-09)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|         | 51/531 [1:40:16<16:55:42, 126.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50: {'policy_loss': np.float64(0.09853548267856241), 'value_loss': np.float64(4.22994108973071), 'kl_divergence': np.float64(0.6958517163991929), 'mean_reward': np.float64(4.470348358154297e-09), 'mean_advantage': np.float64(1.2665987014770507e-08)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  11%|        | 61/531 [2:05:44<22:10:30, 169.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60: {'policy_loss': np.float64(0.1313460787758231), 'value_loss': np.float64(3.741183688491583), 'kl_divergence': np.float64(1.0680326581001283), 'mean_reward': np.float64(-8.195638656616212e-09), 'mean_advantage': np.float64(-2.4959444999694825e-08)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  13%|        | 71/531 [2:26:18<10:14:12, 80.11s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 70: {'policy_loss': np.float64(0.11460776887834072), 'value_loss': np.float64(4.2555758389644325), 'kl_divergence': np.float64(0.96179758310318), 'mean_reward': np.float64(1.0058283805847169e-08), 'mean_advantage': np.float64(-1.564621925354004e-08)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|        | 81/531 [2:52:30<20:40:28, 165.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 80: {'policy_loss': np.float64(0.09744078107178211), 'value_loss': np.float64(3.59176789005287), 'kl_divergence': np.float64(0.9229157567024231), 'mean_reward': np.float64(8.195638656616212e-09), 'mean_advantage': np.float64(1.862645149230957e-08)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  17%|        | 89/531 [3:14:30<21:15:29, 173.14s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train(train_dataloader, num_epochs=1)\n",
    "torch.save(policy_model.state_dict(), \"ppo_trained_model.pth\")\n",
    "print(\"Training complted and model saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
