{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f777a1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2\\\\distilled_gpt2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb108b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6816b98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\Text_models\\\\chat_gpt2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e795087",
   "metadata": {},
   "source": [
    "# Model Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb55165",
   "metadata": {},
   "source": [
    "This Notebook implements the Model Distillation Process for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d82af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from gpt import GPTModelWithHiddenState\n",
    "from model_args import BASE_CONFIG\n",
    "from utils.generate import generate\n",
    "from utils.download_dataset import download_and_load_dataset\n",
    "from utils.token_converter import get_tokenizer, text_to_token_ids, token_ids_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b0809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557a5679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    \"\"\"Format instruction dataset entry into a training prompt\"\"\"\n",
    "    instruction = entry.get(\"instruction\", \"\")\n",
    "    input_text = entry.get(\"input\", \"\")\n",
    "    \n",
    "    if input_text:\n",
    "        # Case where there's both instruction and input\n",
    "        formatted_text = (\n",
    "            f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            f\"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "            f\"### Input:\\n{input_text}\\n\\n\"\n",
    "            f\"### Response:\\n\"\n",
    "        )\n",
    "    else:\n",
    "        # Case where there's only instruction\n",
    "        formatted_text = (\n",
    "            f\"Below is an instruction that describes a task. \"\n",
    "            f\"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            f\"### Instruction:\\n{instruction}\\n\\n\"\n",
    "            f\"### Response:\\n\"\n",
    "        )\n",
    "    \n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf287de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=1024):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        print(f\"Processing {len(data)} examples...\")\n",
    "        \n",
    "        for idx, item in enumerate(data):\n",
    "            try:\n",
    "                # Format the instruction + input as prompt\n",
    "                prompt_text = format_input(item)\n",
    "                \n",
    "                # Get the expected output/response\n",
    "                output_text = item.get(\"output\", \"\")\n",
    "                if not output_text:\n",
    "                    continue  # Skip entries without output\n",
    "                \n",
    "                # Tokenize prompt and full sequence\n",
    "                prompt_tokens = self._safe_tokenize(prompt_text, tokenizer)\n",
    "                full_text = prompt_text + output_text\n",
    "                full_tokens = self._safe_tokenize(full_text, tokenizer)\n",
    "                \n",
    "                # Truncate if too long\n",
    "                if len(full_tokens) > max_length:\n",
    "                    full_tokens = full_tokens[:max_length]\n",
    "                    # Recalculate prompt length after truncation\n",
    "                    prompt_tokens = self._safe_tokenize(prompt_text, tokenizer)\n",
    "                    if len(prompt_tokens) > max_length // 2:\n",
    "                        prompt_tokens = prompt_tokens[:max_length // 2]\n",
    "                \n",
    "                # Create labels (for computing loss only on output tokens)\n",
    "                labels = [-100] * len(prompt_tokens)\n",
    "                \n",
    "                # Add response tokens to labels\n",
    "                if len(full_tokens) > len(prompt_tokens):\n",
    "                    response_tokens = full_tokens[len(prompt_tokens):]\n",
    "                    labels.extend(response_tokens)\n",
    "                \n",
    "                # Ensure labels and input_ids have same length\n",
    "                min_len = min(len(labels), len(full_tokens))\n",
    "                labels = labels[:min_len]\n",
    "                full_tokens = full_tokens[:min_len]\n",
    "                \n",
    "                if len(full_tokens) > 0 and len(labels) > 0:\n",
    "                    self.data.append({\n",
    "                        'input_ids': full_tokens,\n",
    "                        'labels': labels,\n",
    "                        'prompt_length': len(prompt_tokens)\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing item {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Successfully processed {len(self.data)} examples\")\n",
    "    \n",
    "    def _safe_tokenize(self, text, tokenizer):\n",
    "        \"\"\"Safely tokenize text and return as list\"\"\"\n",
    "        try:\n",
    "            # Assuming text_to_token_ids is your tokenization function\n",
    "            tokens = text_to_token_ids(text, tokenizer)\n",
    "            \n",
    "            # Convert to list if it's a tensor\n",
    "            if hasattr(tokens, 'tolist'):\n",
    "                return tokens.tolist()\n",
    "            elif isinstance(tokens, torch.Tensor):\n",
    "                return tokens.cpu().numpy().tolist()\n",
    "            elif isinstance(tokens, (list, tuple)):\n",
    "                return list(tokens)\n",
    "            else:\n",
    "                return [tokens] if isinstance(tokens, int) else []\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12570dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    train_portion = int(len(data) * 0.8)\n",
    "    test_portion = int(len(data) * 0.1)\n",
    "    \n",
    "    train_data = data[:train_portion]\n",
    "    test_data = data[train_portion:train_portion+test_portion]\n",
    "    val_data = data[train_portion+test_portion:]\n",
    "\n",
    "    print(\"train dataset length:\", len(train_data))\n",
    "    print(\"length of test data:\", len(test_data))\n",
    "    print(\"length of val data:\", len(val_data))\n",
    "\n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb75fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_collate_fn(batch, pad_token_id=50256, max_length=1024, device='cpu'):\n",
    "    if not batch:\n",
    "        return None\n",
    "\n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    prompt_lengths = []\n",
    "\n",
    "    for item in batch:\n",
    "        input_ids = torch.tensor(item['input_ids'], dtype=torch.long).flatten()\n",
    "        labels = torch.tensor(item['labels'], dtype=torch.long).flatten()\n",
    "        input_ids_list.append(input_ids)\n",
    "        labels_list.append(labels)\n",
    "        prompt_lengths.append(item['prompt_length'])\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        input_ids_list, batch_first=True, padding_value=pad_token_id\n",
    "    )\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(\n",
    "        labels_list, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    if max_length is not None:\n",
    "        input_ids = input_ids[:, :max_length]\n",
    "        labels = labels[:, :max_length]\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids.to(device),\n",
    "        'labels': labels.to(device),\n",
    "        'prompt_lengths': prompt_lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d405a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_div_loss(T, student_logits, teacher_logits):\n",
    "    \"\"\"Compute KL divergence loss\"\"\"\n",
    "    student_soft = F.log_softmax(student_logits / T, dim=-1)\n",
    "    teacher_soft = F.softmax(teacher_logits / T, dim=-1)\n",
    "    \n",
    "    kl_loss = F.kl_div(\n",
    "        student_soft,\n",
    "        teacher_soft,\n",
    "        reduction='batchmean'\n",
    "    ) * (T ** 2)\n",
    "    \n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78add7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feat_loss(student_hidden_states, teacher_hidden_states):\n",
    "    \"\"\"Compute feature distillation loss between hidden states\"\"\"\n",
    "    if not student_hidden_states or not teacher_hidden_states:\n",
    "        return 0\n",
    "        \n",
    "    if len(student_hidden_states) != len(teacher_hidden_states):\n",
    "        # If different number of layers, use only the minimum\n",
    "        min_layers = min(len(student_hidden_states), len(teacher_hidden_states))\n",
    "        student_hidden_states = student_hidden_states[:min_layers]\n",
    "        teacher_hidden_states = teacher_hidden_states[:min_layers]\n",
    "    \n",
    "    total_loss = 0\n",
    "    valid_layers = 0\n",
    "    \n",
    "    for s_hidden, t_hidden in zip(student_hidden_states, teacher_hidden_states):\n",
    "        # Ensure same shape\n",
    "        if s_hidden.shape != t_hidden.shape:\n",
    "            continue\n",
    "            \n",
    "        cos_sim = F.cosine_similarity(s_hidden, t_hidden, dim=-1)\n",
    "        layer_loss = torch.mean(1 - cos_sim)\n",
    "        total_loss += layer_loss\n",
    "        valid_layers += 1\n",
    "    \n",
    "    return total_loss / valid_layers if valid_layers > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f25a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch, student_model, teacher_model, student_optimizer, config):\n",
    "    \"\"\"Perform one training step for instruction-following dataset\"\"\"\n",
    "    \n",
    "    if batch is None:\n",
    "        return None\n",
    "        \n",
    "    input_ids = batch['input_ids']  # Shape: [batch_size, seq_len]\n",
    "    labels = batch['labels']        # Shape: [batch_size, seq_len]\n",
    "    \n",
    "    # Forward pass through both models\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            teacher_logits, teacher_hidden_states = teacher_model(input_ids)\n",
    "        \n",
    "        student_logits, student_hidden_states = student_model(input_ids)\n",
    "        \n",
    "        # Compute losses\n",
    "        # 1. KL Divergence Loss (knowledge distillation)\n",
    "        kl_loss = kl_div_loss(config.T, student_logits, teacher_logits)\n",
    "        \n",
    "        # 2. Feature Distillation Loss\n",
    "        feat_loss = compute_feat_loss(student_hidden_states, teacher_hidden_states)\n",
    "        \n",
    "        # 3. Cross-entropy loss with labels (only on response tokens)\n",
    "        ce_loss = 0\n",
    "        if labels.numel() > 0:\n",
    "            # Flatten for cross entropy\n",
    "            student_logits_flat = student_logits.view(-1, student_logits.size(-1))\n",
    "            labels_flat = labels.view(-1)\n",
    "            \n",
    "            # Only compute loss on non-ignored tokens (labels != -100)\n",
    "            mask = labels_flat != -100\n",
    "            if mask.any():\n",
    "                ce_loss = F.cross_entropy(\n",
    "                    student_logits_flat[mask], \n",
    "                    labels_flat[mask]\n",
    "                )\n",
    "        \n",
    "        # Combined loss\n",
    "        feat_loss_val = feat_loss if isinstance(feat_loss, torch.Tensor) else torch.tensor(0.0, device=input_ids.device)\n",
    "        ce_loss_val = ce_loss if isinstance(ce_loss, torch.Tensor) else torch.tensor(0.0, device=input_ids.device)\n",
    "        \n",
    "        total_loss = (\n",
    "            config.alpha * ce_loss_val + \n",
    "            (1 - config.alpha) * kl_loss + \n",
    "            config.beta * feat_loss_val\n",
    "        )\n",
    "        \n",
    "        # Backpropagation\n",
    "        student_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        student_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'feat_loss': feat_loss_val.item() if isinstance(feat_loss_val, torch.Tensor) else 0,\n",
    "            'ce_loss': ce_loss_val.item() if isinstance(ce_loss_val, torch.Tensor) else 0\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_step: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ab8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillTrainer:\n",
    "    \"\"\"Knowledge Distillation Trainer\"\"\"\n",
    "    \n",
    "    def __init__(self, config, student_model, teacher_model, tokenizer):\n",
    "        self.config = config\n",
    "        self.student_model = student_model\n",
    "        self.teacher_model = teacher_model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Freeze teacher model\n",
    "        for param in self.teacher_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.student_optimizer = torch.optim.AdamW(\n",
    "            self.student_model.parameters(),\n",
    "            lr=self.config.learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.student_optimizer, T_max=100\n",
    "        )\n",
    "    \n",
    "    def train(self, dataloader, num_epochs=1):\n",
    "        \"\"\"Train the student model using knowledge distillation\"\"\"\n",
    "        \n",
    "        self.student_model.train()\n",
    "        self.teacher_model.eval()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_metrics = []\n",
    "            \n",
    "            progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            \n",
    "            for batch_idx, batch in enumerate(progress_bar):\n",
    "                try:\n",
    "                    metrics = train_step(\n",
    "                        batch,\n",
    "                        self.student_model,\n",
    "                        self.teacher_model,\n",
    "                        self.student_optimizer,\n",
    "                        self.config\n",
    "                    )\n",
    "                    \n",
    "                    if metrics is not None:\n",
    "                        epoch_metrics.append(metrics)\n",
    "                        \n",
    "                        # Update progress bar\n",
    "                        if len(epoch_metrics) > 0:\n",
    "                            avg_loss = np.mean([m['total_loss'] for m in epoch_metrics[-10:]])\n",
    "                            progress_bar.set_postfix({'avg_loss': f'{avg_loss:.4f}'})\n",
    "                        \n",
    "                        # Log detailed metrics every 50 batches\n",
    "                        if batch_idx % 50 == 0 and len(epoch_metrics) >= 10:\n",
    "                            recent_metrics = epoch_metrics[-10:]\n",
    "                            avg_metrics = {\n",
    "                                k: np.mean([m[k] for m in recent_metrics])\n",
    "                                for k in recent_metrics[0].keys()\n",
    "                            }\n",
    "                            print(f\"\\nBatch {batch_idx}: {avg_metrics}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Step scheduler\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Epoch summary\n",
    "            if epoch_metrics:\n",
    "                epoch_avg = {\n",
    "                    k: np.mean([m[k] for m in epoch_metrics])\n",
    "                    for k in epoch_metrics[0].keys()\n",
    "                }\n",
    "                print(f\"\\nEpoch {epoch+1} Summary: {epoch_avg}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}: No successful batches processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed21ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(train_data, test_data, val_data, batch_size, tokenizer, device):\n",
    "    \"\"\"Create data loaders for instruction-following dataset\"\"\"\n",
    "    \n",
    "    # Create custom collate function with device\n",
    "    collate_fn = partial(\n",
    "        distillation_collate_fn,\n",
    "        device=device,\n",
    "        max_length=1024\n",
    "    )\n",
    "    \n",
    "    # Create datasets with max_length parameter\n",
    "    train_dataset = DistillationDataset(train_data, tokenizer, max_length=1024)\n",
    "    test_dataset = DistillationDataset(test_data, tokenizer, max_length=1024)\n",
    "    val_dataset = DistillationDataset(val_data, tokenizer, max_length=1024)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f05dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillConfig:\n",
    "    \"\"\"Configuration for knowledge distillation\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        T=3.0,           # Temperature for knowledge distillation\n",
    "        alpha=0.3,       # Weight for cross-entropy loss\n",
    "        beta=0.1,        # Weight for feature distillation loss\n",
    "        learning_rate=1e-5,\n",
    "        vocab_size=50257,\n",
    "        batch_size=4,\n",
    "        max_length=1024\n",
    "    ):\n",
    "        self.T = T\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.learning_rate = learning_rate\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cbfc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DistillConfig(\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,  # Start small to avoid memory issues\n",
    "    max_length=1024,\n",
    "    T=3.0,\n",
    "    alpha=0.3,\n",
    "    beta=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21ff3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68e097f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModelWithHiddenState(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"gpt_models\\\\SFT_model.pth\"\n",
    "teacher_model = GPTModelWithHiddenState(BASE_CONFIG)\n",
    "teacher_model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device(\"cpu\"),\n",
    "    weights_only=True\n",
    "    )['model_state_dict']\n",
    ")\n",
    "teacher_model.to(device)\n",
    "teacher_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c1813f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModelWithHiddenState(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"gpt_models\\\\Foundational_model.pth\"\n",
    "student_model = GPTModelWithHiddenState(BASE_CONFIG)\n",
    "student_model.load_state_dict(\n",
    "    torch.load(\n",
    "    model_path,\n",
    "    map_location=torch.device('cpu'),\n",
    "    weights_only=True\n",
    "    )\n",
    ")\n",
    "student_model.to(device)\n",
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a98059b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca15094d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 880\n",
      "length of test data: 110\n",
      "length of val data: 110\n",
      "Processing 880 examples...\n",
      "Successfully processed 880 examples\n",
      "Processing 110 examples...\n",
      "Successfully processed 110 examples\n",
      "Processing 110 examples...\n",
      "Successfully processed 110 examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = download_and_load_dataset(\"instruction-data.json\")\n",
    "train_data, test_data, val_data = split_dataset(data)\n",
    "\n",
    "train_dataloader, test_dataloader, val_dataloader = create_dataloaders(\n",
    "    train_data, test_data, val_data, \n",
    "    config.batch_size, tokenizer, device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2b0935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DistillTrainer(config, student_model, teacher_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "622b5f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  23%|██▎       | 51/220 [02:22<10:47,  3.83s/it, avg_loss=139.6129]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 50: {'total_loss': np.float64(139.61294326782226), 'kl_loss': np.float64(199.41669616699218), 'feat_loss': np.float64(0.21258984357118607), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  46%|████▌     | 101/220 [04:39<04:57,  2.50s/it, avg_loss=93.0867]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 100: {'total_loss': np.float64(93.08665161132812), 'kl_loss': np.float64(132.95119705200196), 'feat_loss': np.float64(0.20815364718437196), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  69%|██████▊   | 151/220 [06:52<03:14,  2.81s/it, avg_loss=85.7081] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 150: {'total_loss': np.float64(85.70813751220703), 'kl_loss': np.float64(122.4114891052246), 'feat_loss': np.float64(0.20095255374908447), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  91%|█████████▏| 201/220 [09:07<00:56,  2.97s/it, avg_loss=76.9813]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 200: {'total_loss': np.float64(76.98127250671386), 'kl_loss': np.float64(109.94473037719726), 'feat_loss': np.float64(0.19961598962545396), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 220/220 [10:05<00:00,  2.75s/it, avg_loss=79.8474]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Summary: {'total_loss': np.float64(121.42905696522106), 'kl_loss': np.float64(173.4404061057351), 'feat_loss': np.float64(0.2077528475360437), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  23%|██▎       | 51/220 [02:16<07:09,  2.54s/it, avg_loss=69.1071]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 50: {'total_loss': np.float64(69.10713157653808), 'kl_loss': np.float64(98.69687881469727), 'feat_loss': np.float64(0.19318017661571502), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  46%|████▌     | 101/220 [04:32<05:15,  2.65s/it, avg_loss=67.3134]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 100: {'total_loss': np.float64(67.31340751647949), 'kl_loss': np.float64(96.1349910736084), 'feat_loss': np.float64(0.18915003538131714), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  69%|██████▊   | 151/220 [06:41<03:04,  2.67s/it, avg_loss=49.5978]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 150: {'total_loss': np.float64(49.59783706665039), 'kl_loss': np.float64(70.82784690856934), 'feat_loss': np.float64(0.18344430178403853), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3:  91%|█████████▏| 201/220 [08:53<00:54,  2.88s/it, avg_loss=53.6883]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 200: {'total_loss': np.float64(53.688341522216795), 'kl_loss': np.float64(76.67213249206543), 'feat_loss': np.float64(0.17850395441055297), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 220/220 [09:46<00:00,  2.67s/it, avg_loss=56.8292]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Summary: {'total_loss': np.float64(60.624206126819956), 'kl_loss': np.float64(86.57930526733398), 'feat_loss': np.float64(0.18693537122823975), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  23%|██▎       | 51/220 [02:16<06:15,  2.22s/it, avg_loss=48.6724]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 50: {'total_loss': np.float64(48.67239990234375), 'kl_loss': np.float64(69.5071014404297), 'feat_loss': np.float64(0.1742960423231125), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  46%|████▌     | 101/220 [04:52<06:25,  3.24s/it, avg_loss=46.4936]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 100: {'total_loss': np.float64(46.493560791015625), 'kl_loss': np.float64(66.3950912475586), 'feat_loss': np.float64(0.16996641159057618), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  69%|██████▊   | 151/220 [07:02<02:49,  2.45s/it, avg_loss=54.7707]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 150: {'total_loss': np.float64(54.77074508666992), 'kl_loss': np.float64(78.21990585327148), 'feat_loss': np.float64(0.16812762022018432), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3:  91%|█████████▏| 201/220 [08:58<00:38,  2.05s/it, avg_loss=45.5694]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 200: {'total_loss': np.float64(45.569393157958984), 'kl_loss': np.float64(65.07554397583007), 'feat_loss': np.float64(0.16513274163007735), 'ce_loss': np.float64(0.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 220/220 [09:40<00:00,  2.64s/it, avg_loss=46.4272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Summary: {'total_loss': np.float64(47.3233569318598), 'kl_loss': np.float64(67.5805571642789), 'feat_loss': np.float64(0.16967466670003803), 'ce_loss': np.float64(0.0)}\n",
      "-------------------------------------------------------------------\n",
      "\n",
      "Training has been completed and model has beed saved\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "\n",
    "trainer.train(train_dataloader, num_epochs=3)\n",
    "torch.save({\n",
    "    'model_state_dict': student_model.state_dict(),\n",
    "    'config': config.__dict__,\n",
    "    'epoch': 3\n",
    "}, 'distilled_gpt_model.pth')\n",
    "print(\"-------------------------------------------------------------------\")\n",
    "print()\n",
    "print(\"Training has been completed and model has beed saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876a08d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
