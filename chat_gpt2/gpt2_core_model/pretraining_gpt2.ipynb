{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\chat_gpt2\\\\gpt2_core_model'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\software_3\\\\Generative_models\\\\chat_gpt2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import torch \n",
    "import tiktoken\n",
    "import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from  torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gpt-2 implementation is based on this research paper [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's the architecture of the GPT-2\n",
    "\n",
    "![GPT-2 image](https://camo.githubusercontent.com/6c8c392f72d5b9e86c94aeb9470beab435b888d24135926f1746eb88e0cc18fb/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830345f636f6d707265737365642f31332e776562703f31)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, drop_rate, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        queries = self.w_query(x)\n",
    "        keys = self.w_key(x)\n",
    "        values = self.w_value(x)\n",
    "\n",
    "        # [batch, tokens, heads, head_dim]\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # [batch, heads, tokens, head_dim]\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores = attn_scores.masked_fill(mask_bool, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / (self.head_dim ** 0.5), dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        return self.out_proj(context_vec)\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.gelu(x, approximate=\"tanh\")\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            drop_rate=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = nn.LayerNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.norm2 = nn.LayerNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"], eps=1e-5)\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "        # Weight tying: share weights between token embedding and output head\n",
    "        self.out_head.weight = self.tok_emb.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len, device=in_idx.device)).unsqueeze(0)\n",
    "        x = tok_embeds + pos_emb\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL_ARGS = {\n",
    "    \"emb_dim\": 768,\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"n_layers\": 12,\n",
    "    \"n_heads\": 12,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (w_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (w_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_MODEL_ARGS)\n",
    "model.to(get_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization / Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    token = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    token_tensor = torch.tensor(token).unsqueeze(0)\n",
    "    return token_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text / Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "        model,\n",
    "        idx, max_new_tokens,\n",
    "        context_size,\n",
    "        temperature=0.0,\n",
    "        top_k=None,\n",
    "        eos_id=None\n",
    "    ):\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float(\"-inf\")).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probas = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probas, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        if  eos_id is not None and (idx_next == eos_id).all():\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "\n",
    "def generate_and_print_text(\n",
    "    model,\n",
    "    tokenizer, \n",
    "    device,\n",
    "    start_context\n",
    "    ):\n",
    "\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[1]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate(\n",
    "            model=model,\n",
    "            idx=encoded,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset load/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_load_dataset(file):\n",
    "    download_file_path = os.getcwd()\n",
    "    download_file_path = os.path.abspath(os.path.join(download_file_path, '.'))\n",
    "    if not os.path.exists(file):\n",
    "        if file.startswith(\"https\"):\n",
    "            with urllib.request.urlopen(file) as response:\n",
    "                text_data = response.read().decode(\"utf-8\")\n",
    "\n",
    "            filename = os.path.basename(file) or \"downloaded_file.txt\"\n",
    "            full_path = os.path.join(download_file_path, filename)\n",
    "\n",
    "            with open(full_path, \"w\", encoding=\"utf-8\") as data_file:\n",
    "                data_file.write(text_data)\n",
    "\n",
    "            # Load as plain text\n",
    "            with open(full_path, \"r\", encoding=\"utf-8\") as data_file:\n",
    "                data = data_file.read()\n",
    "            return data\n",
    "        else:\n",
    "            raise FileNotFoundError(\"File does not exist. Please provide a valid data_file_path or URL.\")\n",
    "    else:\n",
    "        \n",
    "        if file.endswith(\".json\"):\n",
    "            import json\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as data_file:\n",
    "                data = json.load(data_file)\n",
    "        else:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as data_file:\n",
    "                data = data_file.read()\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "def partition_data(data):\n",
    "    n = len(data)\n",
    "    train_portion = int(n * 0.85)\n",
    "    test_portion = int(n * 0.10)\n",
    "    val_portion = n - train_portion - test_portion\n",
    "\n",
    "    train_data = data[:train_portion]\n",
    "    test_data = data[train_portion:train_portion + test_portion]\n",
    "    val_data = data[train_portion + test_portion:]\n",
    "\n",
    "    print(\"Training data length:\", len(train_data))\n",
    "    print(\"Test data length:\", len(test_data))\n",
    "    print(\"Validation data length:\", len(val_data))\n",
    "\n",
    "    return train_data, test_data, val_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining GPTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainingDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        \n",
    "        token_ids = tokenizer.encode(data)\n",
    "        \n",
    "        if len(token_ids) < max_length:\n",
    "            raise ValueError(f\"Data is too short ({len(token_ids)} tokens). Needs at least {max_length} tokens.\")\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1:i+max_length+1]\n",
    "            \n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "\n",
    "def create_dataloaders(\n",
    "        data,\n",
    "        tokenizer,\n",
    "        batch_size=10,\n",
    "        max_length=256,\n",
    "        stride=128,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=0\n",
    "    ):\n",
    "\n",
    "    dataset = PretrainingDataset(data, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            drop_last=drop_last,\n",
    "                            num_workers=num_workers)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss_per_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    logits = model(input_batch) \n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)), target_batch.view(-1)\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = cal_loss_per_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(\n",
    "            train_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "        val_loss = calc_loss_loader(\n",
    "            val_loader, model, device, num_batches=eval_iter\n",
    "        )\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        num_epochs,\n",
    "        eval_freq,\n",
    "        eval_iter,\n",
    "        start_context,\n",
    "        tokenizer\n",
    "    ):\n",
    "    train_losses, val_losses, track_token_seen = [], [], []\n",
    "    token_seen, global_step = 0, -1\n",
    "\n",
    "    accumulation_step = 4\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=True)\n",
    "        for i, (input_batch, target_batch) in enumerate(pbar):\n",
    "            loss = cal_loss_per_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "\n",
    "            loss = loss / accumulation_step\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % accumulation_step == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            token_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_token_seen.append(token_seen)\n",
    "                pbar.set_postfix({\n",
    "                    \"train_loss\": f\"{train_loss:.3f}\",\n",
    "                    \"val_loss\": f\"{val_loss:.3f}\"\n",
    "                })\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"\\n[Epoch {epoch+1}] Sample Generation:\")\n",
    "        generate_and_print_text(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            start_context\n",
    "        )\n",
    "    print(\"Model training has been completed.\")\n",
    "    #    return train_losses, val_losses, start_context\n",
    "    return train_losses, val_losses, track_token_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, optimizer):\n",
    "    save_dir = os.getcwd()\n",
    "    save_dir = os.path.join(os.path.abspath(os.path.join(save_dir, '.')), \"gpt_models\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, \"GPT2-355M.pth\")\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }, save_path)\n",
    "    print(f\"Pretrained model has been saved successfully at {save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrained_model(model, model_args, device, model_name=\"GPT2-355M\"):\n",
    "\n",
    "    save_dir = os.getcwd()\n",
    "    save_dir = os.path.join(os.path.abspath(os.path.join(save_dir, '.')), \"gpt_models\")\n",
    "    model_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model = model(model_args)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(f\"Loaded pretrained model from {model_path}\")\n",
    "        return model\n",
    "    else:\n",
    "        raise FileNotFoundError(\"There is no pretrained model. You need to train the model first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_gpt2(\n",
    "        model,\n",
    "        device,\n",
    "        tokenizer,\n",
    "        optimizer,\n",
    "        train_data,\n",
    "        val_data,\n",
    "        batch_size,\n",
    "        num_epochs,\n",
    "        start_context\n",
    "):\n",
    "    train_loader = create_dataloaders(\n",
    "        train_data,\n",
    "        tokenizer,\n",
    "        batch_size=batch_size,\n",
    "        max_length=GPT_MODEL_ARGS[\"context_length\"],\n",
    "        stride=GPT_MODEL_ARGS[\"context_length\"],\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = create_dataloaders(\n",
    "        val_data,\n",
    "        tokenizer,\n",
    "        batch_size=batch_size,\n",
    "        max_length=GPT_MODEL_ARGS[\"context_length\"],\n",
    "        stride=GPT_MODEL_ARGS[\"context_length\"],\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    print(\"Initializing training...\")\n",
    "    train_losses, val_losses, tokens_seen = pre_train_model(\n",
    "        model, train_loader, val_loader, optimizer, device, \n",
    "        num_epochs, eval_freq=5, eval_iter=5,\n",
    "        start_context=start_context, tokenizer=tokenizer\n",
    "    )\n",
    "    print(\"Pre-training complete.\")\n",
    "\n",
    "    save_model(model, optimizer)  \n",
    "    print(\"Pre-trained model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\rppaw\\AppData\\Local\\Temp\\ipykernel_16536\\3838271407.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  data_path = \"D:\\software_3\\\\Generative_models\\\\chat_gpt2\\\\input.txt\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tokenizer loaded\n",
      "Dataset loaded\n",
      "Training data length: 471840\n",
      "Test data length: 55510\n",
      "Validation data length: 27757\n",
      "Initializing training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 68/68 [15:26<00:00, 13.63s/it, train_loss=6.419, val_loss=6.597] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1] Sample Generation:\n",
      "Else might I think that Clarence, Edward's brother                                                  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  10%|█         | 7/68 [02:02<17:47, 17.49s/it, train_loss=6.413, val_loss=6.557]\n",
      "C:\\Users\\rppaw\\AppData\\Local\\Temp\\ipykernel_16536\\3838271407.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  data_path = \"D:\\software_3\\\\Generative_models\\\\chat_gpt2\\\\input.txt\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     18\u001b[39m     num_epochs = \u001b[32m2\u001b[39m\n\u001b[32m     19\u001b[39m     start_context = \u001b[33m\"\u001b[39m\u001b[33mElse might I think that Clarence, Edward\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms brother\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[43mpretrain_gpt2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_context\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[33;03m\"\"\"Were but a feigned friend to our proceedings:\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    But welcome, sweet Clarence; my daughter shall be thine.\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    And now what rests but, in night's coverture,\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    Thy brother being carelessly encamp'd, \"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mpretrain_gpt2\u001b[39m\u001b[34m(model, device, tokenizer, optimizer, train_data, val_data, batch_size, num_epochs, start_context)\u001b[39m\n\u001b[32m     23\u001b[39m val_loader = create_dataloaders(\n\u001b[32m     24\u001b[39m     val_data,\n\u001b[32m     25\u001b[39m     tokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     31\u001b[39m     num_workers=\u001b[32m0\u001b[39m\n\u001b[32m     32\u001b[39m )\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitializing training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m train_losses, val_losses, tokens_seen = \u001b[43mpre_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPre-training complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     42\u001b[39m save_model(model, optimizer)  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mpre_train_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_step % eval_freq == \u001b[32m0\u001b[39m:\n\u001b[32m     38\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     train_loss, val_loss = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     train_losses.append(train_loss)\n\u001b[32m     44\u001b[39m     val_losses.append(val_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, train_loader, val_loader, device, eval_iter)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     30\u001b[39m     train_loss = calc_loss_loader(\n\u001b[32m     31\u001b[39m         train_loader, model, device, num_batches=eval_iter\n\u001b[32m     32\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     val_loss = \u001b[43mcalc_loss_loader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_iter\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m model.train()\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss, val_loss\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mcalc_loss_loader\u001b[39m\u001b[34m(data_loader, model, device, num_batches)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i < num_batches:\n\u001b[32m     21\u001b[39m     loss = cal_loss_per_batch(input_batch, target_batch, model, device)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model = GPTModel(GPT_MODEL_ARGS)\n",
    "    device = get_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = model.to(device)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    print(\"Tokenizer loaded\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "    \n",
    "   \n",
    "    data_path = \"D:\\software_3\\\\Generative_models\\\\chat_gpt2\\\\input.txt\"\n",
    "    #\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "    data = download_and_load_dataset(data_path)\n",
    "    print(\"Dataset loaded\")\n",
    "    train_data, _, val_data = partition_data(data)\n",
    "    batch_size = 2\n",
    "    num_epochs = 2\n",
    "    start_context = \"Else might I think that Clarence, Edward's brother\"\n",
    "    \n",
    "    \n",
    "\n",
    "    pretrain_gpt2(\n",
    "        model,\n",
    "        device,\n",
    "        tokenizer,\n",
    "        optimizer,\n",
    "        train_data,\n",
    "        val_data,\n",
    "        batch_size,\n",
    "        num_epochs,\n",
    "        start_context\n",
    "    )\n",
    "\n",
    "\"\"\"Were but a feigned friend to our proceedings:\n",
    "    But welcome, sweet Clarence; my daughter shall be thine.\n",
    "    And now what rests but, in night's coverture,\n",
    "    Thy brother being carelessly encamp'd, \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model from d:\\software_3\\Generative_models\\Text_models\\gpt2\\gpt_models\\PRETRAINED_GPT_MODEL.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GPTModel\n",
    "model_args = GPT_MODEL_ARGS\n",
    "device =get_device() \n",
    "model = load_pretrained_model(model, model_args, device, model_name=\"GPT2-355M\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hath pawn'd an open hand in sign of love;\n",
      "Else might I think that Clarence, Edward's brother,\n",
      "Were but a feigned friend to our proceedings:\n",
      "It is a shame\n",
      "But now noly honour of the world or one\n",
      "But in the end;\n",
      "I have him the world,\n",
      "And I say I do fear him,\n",
      "And will be decres at the presence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Hath pawn'd an open hand in sign of love;\n",
    "Else might I think that Clarence, Edward's brother,\n",
    "Were but a feigned friend to our proceedings:\"\"\"\n",
    "\n",
    "encoded_text =  text_to_token_ids(text, tokenizer).to(device)\n",
    "\n",
    "idx = encoded_text\n",
    "token_ids = generate(\n",
    "        model=model,\n",
    "        idx=encoded_text,\n",
    "        max_new_tokens=50,\n",
    "        context_size=GPT_MODEL_ARGS[\"context_length\"],\n",
    "        temperature=0.0,\n",
    "        top_k=None,\n",
    "        eos_id=None\n",
    "    )\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
