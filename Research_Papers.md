# Introduction

This are the lists of important research papers for building and understanding gpt2 model and the training processes.

### **gpt2**

- [Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)


### **Supervised Fine Tunning**

- [Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/abs/2403.14608)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

### **Preference Alignment**

#### *Direct Prefernce Alignment**

- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)

#### **Reinforcement learning with human Feedback**

- [Reinforcement Learning from Human Feedback (Comprehensive Book)](https://arxiv.org/abs/2504.12501)
- [Reinforcement Learning: An Introduction (book)](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)

#### **Reward Model**

- [Bradleyâ€“Terry and Multi-Objective Reward Modeling Are Complementary](https://arxiv.org/html/2507.07375v1)
- [Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives](https://arxiv.org/abs/2411.04991)
- [Rethinking Reward Modeling in Preference-based Large Language Model Alignment](https://openreview.net/forum?id=rfdblE10qm)
- [The Lessons of Developing Process Reward Models
in Mathematical Reasoning](https://arxiv.org/abs/2501.07301)

#### **Proximal Policy Optimization (PPO)**

- [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)
- [Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment](https://arxiv.org/abs/2310.00212)
- [Secrets of RLHF in Large Language Models Part I: PPO](http://arxiv.org/abs/2307.04964)
- [HIGH-DIMENSIONAL CONTINUOUS CONTROL USING
GENERALIZED ADVANTAGE ESTIMATION (GAE)](https://arxiv.org/pdf/1506.02438)
- [Trust Region Policy Optimization (Trpo)](https://arxiv.org/pdf/1502.05477)
- [Kullback-Leibler divergence between quantum distributions, and its upper-bound](https://arxiv.org/abs/2008.05932)
- [The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization](http://arxiv.org/abs/2403.17031)

#### **Group Relative Policy Optimization(GRPO)**

- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](http://arxiv.org/abs/2402.03300)
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948)
- [Kimi k1.5: Scaling Reinforcement Learning with LLMs](https://arxiv.org/abs//2501.12599)
- [KIMI K2: OPEN AGENTIC INTELLIGENCE](https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf)
- [Dota 2 with Large Scale Deep Reinforcement Learning](http://arxiv.org/abs/1912.06680)


### **Knowledge Distillation**


- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948)
- [Gemma 3 Technical Report](http://arxiv.org/abs/2503.19786)
- [Distilling the Knowledge in a Neural Network](http://arxiv.org/abs/1503.02531)
- [Gemma 2: Improving Open Language Models at a Practical Size](http://arxiv.org/abs/2408.00118)
- [MiniLLM: Knowledge Distillation of Large Language Models](http://arxiv.org/abs/2306.08543)
- [Gemma: Open Models Based on Gemini Research and Technology](http://arxiv.org/abs/2403.08295)
- [On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes](http://arxiv.org/abs/2306.13649)
- [Well-Read Students Learn Better: On the Importance of Pre-training Compact Models](http://arxiv.org/abs/1908.08962)
- [Tailoring Language Generation Models under Total Variation Distance](http://arxiv.org/abs/2302.13344)
- [Distilling Policy Distillation](http://arxiv.org/abs/1902.02186)
- [Policy Distillation](http://arxiv.org/abs/1511.06295)
- [Knowledge Distillation: A Survey](http://arxiv.org/abs/2006.05525)
- [Sequence-Level Knowledge Distillation](http://arxiv.org/abs/1606.07947)
- [Model compression](https://dl.acm.org/doi/10.1145/1150402.1150464)
- [Teaching language models to support answers with verified quotes](http://arxiv.org/abs/2203.11147)
- [Self-Rewarding Language Models](http://arxiv.org/abs/2401.10020)
- [AlignDistil: Token-Level Language Model Alignment as Adaptive Policy Distillation](http://arxiv.org/abs/2503.02832)
- [Autoregressive Knowledge Distillation through Imitation Learning](http://arxiv.org/abs/2009.07253)